<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Chat Models &mdash; 🦜🔗 LangChain 0.0.190</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/autodoc_pydantic.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
      <link rel="stylesheet" href="../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" type="text/css" />
      <link rel="stylesheet" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/js/mendablesearch.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Output Parsers" href="output_parsers.html" />
    <link rel="prev" title="LLMs" href="llms.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            🦜🔗 LangChain
          </a>
              <div class="version">
                0.0.190
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Abstractions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="base_classes.html">Base classes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../model_io.html">Model I/O</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../prompts.html">Prompts</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../models.html">Models</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="llms.html">LLMs</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Chat Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="output_parsers.html">Output Parsers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../data_connection.html">Data connection</a></li>
<li class="toctree-l1"><a class="reference internal" href="chains.html">Chains</a></li>
<li class="toctree-l1"><a class="reference internal" href="../agents.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory.html">Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="callbacks.html">Callbacks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="utilities.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="experimental.html">Experimental</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">🦜🔗 LangChain</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../model_io.html">Model I/O</a></li>
          <li class="breadcrumb-item"><a href="../models.html">Models</a></li>
      <li class="breadcrumb-item active">Chat Models</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/hwchase17/langchain/blob/master/docs/api_referencemodules/chat_models.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-langchain.chat_models">
<span id="chat-models"></span><h1>Chat Models<a class="headerlink" href="#module-langchain.chat_models" title="Permalink to this headline"></a></h1>
<dl class="py class pydantic_model">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">langchain.chat_models.</span></span><span class="sig-name descname"><span class="pre">ChatOpenAI</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback_manager</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">client</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gpt-3.5-turbo'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_api_key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_api_base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_organization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_proxy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request_timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_retries</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streaming</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/langchain/chat_models/openai.html#ChatOpenAI"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain.chat_models.ChatOpenAI" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">langchain.chat_models.base.BaseChatModel</span></code></p>
<p>Wrapper around OpenAI Chat large language models.</p>
<p>To use, you should have the <code class="docutils literal notranslate"><span class="pre">openai</span></code> python package installed, and the
environment variable <code class="docutils literal notranslate"><span class="pre">OPENAI_API_KEY</span></code> set with your API key.</p>
<p>Any parameters that are valid to be passed to the openai.create call can be passed
in, even if not explicitly saved on this class.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="n">openai</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>verbose</strong> (<em>bool</em>) – </p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>langchain.callbacks.base.BaseCallbackHandler</em><em>]</em><em>, </em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>callback_manager</strong> (<em>Optional</em><em>[</em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em>) – </p></li>
<li><p><strong>client</strong> (<em>Any</em>) – </p></li>
<li><p><strong>model</strong> (<em>str</em>) – </p></li>
<li><p><strong>temperature</strong> (<em>float</em>) – </p></li>
<li><p><strong>model_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – </p></li>
<li><p><strong>openai_api_key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>openai_api_base</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>openai_organization</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>openai_proxy</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>request_timeout</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>float</em><em>, </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>max_retries</strong> (<em>int</em>) – </p></li>
<li><p><strong>streaming</strong> (<em>bool</em>) – </p></li>
<li><p><strong>n</strong> (<em>int</em>) – </p></li>
<li><p><strong>max_tokens</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.max_retries">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_retries</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">6</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.max_retries" title="Permalink to this definition"></a></dt>
<dd><p>Maximum number of retries to make when generating.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.max_tokens">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_tokens</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.max_tokens" title="Permalink to this definition"></a></dt>
<dd><p>Maximum number of tokens to generate.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.model_kwargs">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><em class="property"> <span class="pre">[Optional]</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.model_kwargs" title="Permalink to this definition"></a></dt>
<dd><p>Holds any model parameters valid for <cite>create</cite> call not explicitly specified.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.model_name">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'gpt-3.5-turbo'</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'model')</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.model_name" title="Permalink to this definition"></a></dt>
<dd><p>Model name to use.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.n">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.n" title="Permalink to this definition"></a></dt>
<dd><p>Number of chat completions to generate for each prompt.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.openai_api_base">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_api_base</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.openai_api_base" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.openai_api_key">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_api_key</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.openai_api_key" title="Permalink to this definition"></a></dt>
<dd><p>Base URL path for API requests,
leave blank if not using a proxy or service emulator.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.openai_organization">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_organization</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.openai_organization" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.openai_proxy">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_proxy</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.openai_proxy" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.request_timeout">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">request_timeout</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.request_timeout" title="Permalink to this definition"></a></dt>
<dd><p>Timeout for requests to OpenAI completion API. Default is 600 seconds.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.streaming">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">streaming</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.streaming" title="Permalink to this definition"></a></dt>
<dd><p>Whether to stream the results or not.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.temperature">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">temperature</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.7</span></em><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.temperature" title="Permalink to this definition"></a></dt>
<dd><p>What sampling temperature to use.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.completion_with_retry">
<span class="sig-name descname"><span class="pre">completion_with_retry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/langchain/chat_models/openai.html#ChatOpenAI.completion_with_retry"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.completion_with_retry" title="Permalink to this definition"></a></dt>
<dd><p>Use tenacity to retry the completion call.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>kwargs</strong> (<em>Any</em>) – </p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><em>Any</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.get_num_tokens_from_messages">
<span class="sig-name descname"><span class="pre">get_num_tokens_from_messages</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">messages</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/langchain/chat_models/openai.html#ChatOpenAI.get_num_tokens_from_messages"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.get_num_tokens_from_messages" title="Permalink to this definition"></a></dt>
<dd><p>Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.</p>
<p>Official documentation: <a class="reference external" href="https://github.com/openai/openai-cookbook/blob/">https://github.com/openai/openai-cookbook/blob/</a>
main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>messages</strong> (<em>List</em><em>[</em><a class="reference internal" href="base_classes.html#langchain.schema.BaseMessage" title="langchain.schema.BaseMessage"><em>langchain.schema.BaseMessage</em></a><em>]</em>) – </p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="langchain.chat_models.ChatOpenAI.get_token_ids">
<span class="sig-name descname"><span class="pre">get_token_ids</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/langchain/chat_models/openai.html#ChatOpenAI.get_token_ids"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain.chat_models.ChatOpenAI.get_token_ids" title="Permalink to this definition"></a></dt>
<dd><p>Get the tokens present in the text with tiktoken package.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>) – </p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><em>List</em>[int]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class pydantic_model">
<dt class="sig sig-object py" id="langchain.chat_models.AzureChatOpenAI">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">langchain.chat_models.</span></span><span class="sig-name descname"><span class="pre">AzureChatOpenAI</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback_manager</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">client</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gpt-3.5-turbo'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_api_key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_api_base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_organization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_proxy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request_timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_retries</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streaming</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deployment_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_api_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'azure'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_api_version</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/langchain/chat_models/azure_openai.html#AzureChatOpenAI"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain.chat_models.AzureChatOpenAI" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#langchain.chat_models.ChatOpenAI" title="langchain.chat_models.openai.ChatOpenAI"><code class="xref py py-class docutils literal notranslate"><span class="pre">langchain.chat_models.openai.ChatOpenAI</span></code></a></p>
<p>Wrapper around Azure OpenAI Chat Completion API. To use this class you
must have a deployed model on Azure OpenAI. Use <cite>deployment_name</cite> in the
constructor to refer to the “Model deployment name” in the Azure portal.</p>
<p>In addition, you should have the <code class="docutils literal notranslate"><span class="pre">openai</span></code> python package installed, and the
following environment variables set or passed in constructor in lower case:
- <code class="docutils literal notranslate"><span class="pre">OPENAI_API_TYPE</span></code> (default: <code class="docutils literal notranslate"><span class="pre">azure</span></code>)
- <code class="docutils literal notranslate"><span class="pre">OPENAI_API_KEY</span></code>
- <code class="docutils literal notranslate"><span class="pre">OPENAI_API_BASE</span></code>
- <code class="docutils literal notranslate"><span class="pre">OPENAI_API_VERSION</span></code>
- <code class="docutils literal notranslate"><span class="pre">OPENAI_PROXY</span></code></p>
<p>For exmaple, if you have <cite>gpt-35-turbo</cite> deployed, with the deployment name
<cite>35-turbo-dev</cite>, the constructor should look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AzureChatOpenAI</span><span class="p">(</span>
    <span class="n">deployment_name</span><span class="o">=</span><span class="s2">&quot;35-turbo-dev&quot;</span><span class="p">,</span>
    <span class="n">openai_api_version</span><span class="o">=</span><span class="s2">&quot;2023-03-15-preview&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Be aware the API version may change.</p>
<p>Any parameters that are valid to be passed to the openai.create call can be passed
in, even if not explicitly saved on this class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>verbose</strong> (<em>bool</em>) – </p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>langchain.callbacks.base.BaseCallbackHandler</em><em>]</em><em>, </em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>callback_manager</strong> (<em>Optional</em><em>[</em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em>) – </p></li>
<li><p><strong>client</strong> (<em>Any</em>) – </p></li>
<li><p><strong>model</strong> (<em>str</em>) – </p></li>
<li><p><strong>temperature</strong> (<em>float</em>) – </p></li>
<li><p><strong>model_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – </p></li>
<li><p><strong>openai_api_key</strong> (<em>str</em>) – </p></li>
<li><p><strong>openai_api_base</strong> (<em>str</em>) – </p></li>
<li><p><strong>openai_organization</strong> (<em>str</em>) – </p></li>
<li><p><strong>openai_proxy</strong> (<em>str</em>) – </p></li>
<li><p><strong>request_timeout</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>float</em><em>, </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>max_retries</strong> (<em>int</em>) – </p></li>
<li><p><strong>streaming</strong> (<em>bool</em>) – </p></li>
<li><p><strong>n</strong> (<em>int</em>) – </p></li>
<li><p><strong>max_tokens</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – </p></li>
<li><p><strong>deployment_name</strong> (<em>str</em>) – </p></li>
<li><p><strong>openai_api_type</strong> (<em>str</em>) – </p></li>
<li><p><strong>openai_api_version</strong> (<em>str</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.AzureChatOpenAI.deployment_name">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">deployment_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">''</span></em><a class="headerlink" href="#langchain.chat_models.AzureChatOpenAI.deployment_name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.AzureChatOpenAI.openai_api_base">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_api_base</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">''</span></em><a class="headerlink" href="#langchain.chat_models.AzureChatOpenAI.openai_api_base" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.AzureChatOpenAI.openai_api_key">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_api_key</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">''</span></em><a class="headerlink" href="#langchain.chat_models.AzureChatOpenAI.openai_api_key" title="Permalink to this definition"></a></dt>
<dd><p>Base URL path for API requests,
leave blank if not using a proxy or service emulator.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.AzureChatOpenAI.openai_api_type">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_api_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'azure'</span></em><a class="headerlink" href="#langchain.chat_models.AzureChatOpenAI.openai_api_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.AzureChatOpenAI.openai_api_version">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_api_version</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">''</span></em><a class="headerlink" href="#langchain.chat_models.AzureChatOpenAI.openai_api_version" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.AzureChatOpenAI.openai_organization">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_organization</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">''</span></em><a class="headerlink" href="#langchain.chat_models.AzureChatOpenAI.openai_organization" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.AzureChatOpenAI.openai_proxy">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_proxy</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">''</span></em><a class="headerlink" href="#langchain.chat_models.AzureChatOpenAI.openai_proxy" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class pydantic_model">
<dt class="sig sig-object py" id="langchain.chat_models.PromptLayerChatOpenAI">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">langchain.chat_models.</span></span><span class="sig-name descname"><span class="pre">PromptLayerChatOpenAI</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback_manager</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">client</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gpt-3.5-turbo'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_api_key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_api_base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_organization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openai_proxy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request_timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_retries</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streaming</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_tags</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_pl_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/langchain/chat_models/promptlayer_openai.html#PromptLayerChatOpenAI"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain.chat_models.PromptLayerChatOpenAI" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#langchain.chat_models.ChatOpenAI" title="langchain.chat_models.openai.ChatOpenAI"><code class="xref py py-class docutils literal notranslate"><span class="pre">langchain.chat_models.openai.ChatOpenAI</span></code></a></p>
<p>Wrapper around OpenAI Chat large language models and PromptLayer.</p>
<p>To use, you should have the <code class="docutils literal notranslate"><span class="pre">openai</span></code> and <code class="docutils literal notranslate"><span class="pre">promptlayer</span></code> python
package installed, and the environment variable <code class="docutils literal notranslate"><span class="pre">OPENAI_API_KEY</span></code>
and <code class="docutils literal notranslate"><span class="pre">PROMPTLAYER_API_KEY</span></code> set with your openAI API key and
promptlayer key respectively.</p>
<p>All parameters that can be passed to the OpenAI LLM can also
be passed here. The PromptLayerChatOpenAI adds to optional</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pl_tags</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – List of strings to tag the request with.</p></li>
<li><p><strong>return_pl_id</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – If True, the PromptLayer request ID will be
returned in the <code class="docutils literal notranslate"><span class="pre">generation_info</span></code> field of the
<code class="docutils literal notranslate"><span class="pre">Generation</span></code> object.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – </p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>langchain.callbacks.base.BaseCallbackHandler</em><em>]</em><em>, </em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>callback_manager</strong> (<em>Optional</em><em>[</em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em>) – </p></li>
<li><p><strong>client</strong> (<em>Any</em>) – </p></li>
<li><p><strong>model</strong> (<em>str</em>) – </p></li>
<li><p><strong>temperature</strong> (<em>float</em>) – </p></li>
<li><p><strong>model_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – </p></li>
<li><p><strong>openai_api_key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>openai_api_base</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>openai_organization</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>openai_proxy</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>request_timeout</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>float</em><em>, </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>max_retries</strong> (<em>int</em>) – </p></li>
<li><p><strong>streaming</strong> (<em>bool</em>) – </p></li>
<li><p><strong>n</strong> (<em>int</em>) – </p></li>
<li><p><strong>max_tokens</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.chat_models</span> <span class="kn">import</span> <span class="n">PromptLayerChatOpenAI</span>
<span class="n">openai</span> <span class="o">=</span> <span class="n">PromptLayerChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.PromptLayerChatOpenAI.pl_tags">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">pl_tags</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.PromptLayerChatOpenAI.pl_tags" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.PromptLayerChatOpenAI.return_pl_id">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">return_pl_id</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain.chat_models.PromptLayerChatOpenAI.return_pl_id" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class pydantic_model">
<dt class="sig sig-object py" id="langchain.chat_models.ChatAnthropic">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">langchain.chat_models.</span></span><span class="sig-name descname"><span class="pre">ChatAnthropic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">client</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'claude-v1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens_to_sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streaming</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_request_timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">anthropic_api_key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">HUMAN_PROMPT</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">AI_PROMPT</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback_manager</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/langchain/chat_models/anthropic.html#ChatAnthropic"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain.chat_models.ChatAnthropic" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">langchain.chat_models.base.BaseChatModel</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">langchain.llms.anthropic._AnthropicCommon</span></code></p>
<p>Wrapper around Anthropic’s large language model.</p>
<p>To use, you should have the <code class="docutils literal notranslate"><span class="pre">anthropic</span></code> python package installed, and the
environment variable <code class="docutils literal notranslate"><span class="pre">ANTHROPIC_API_KEY</span></code> set with your API key, or pass
it as a named parameter to the constructor.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">anthropic</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">Anthropic</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;&lt;model_name&gt;&quot;</span><span class="p">,</span> <span class="n">anthropic_api_key</span><span class="o">=</span><span class="s2">&quot;my-api-key&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>client</strong> (<em>Any</em>) – </p></li>
<li><p><strong>model</strong> (<em>str</em>) – </p></li>
<li><p><strong>max_tokens_to_sample</strong> (<em>int</em>) – </p></li>
<li><p><strong>temperature</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – </p></li>
<li><p><strong>top_k</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – </p></li>
<li><p><strong>top_p</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – </p></li>
<li><p><strong>streaming</strong> (<em>bool</em>) – </p></li>
<li><p><strong>default_request_timeout</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>float</em><em>, </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>anthropic_api_key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>HUMAN_PROMPT</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>AI_PROMPT</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>count_tokens</strong> (<em>Optional</em><em>[</em><em>Callable</em><em>[</em><em>[</em><em>str</em><em>]</em><em>, </em><em>int</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – </p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>langchain.callbacks.base.BaseCallbackHandler</em><em>]</em><em>, </em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>callback_manager</strong> (<em>Optional</em><em>[</em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain.chat_models.ChatAnthropic.get_num_tokens">
<span class="sig-name descname"><span class="pre">get_num_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/langchain/chat_models/anthropic.html#ChatAnthropic.get_num_tokens"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain.chat_models.ChatAnthropic.get_num_tokens" title="Permalink to this definition"></a></dt>
<dd><p>Calculate number of tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>) – </p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class pydantic_model">
<dt class="sig sig-object py" id="langchain.chat_models.ChatGooglePalm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">langchain.chat_models.</span></span><span class="sig-name descname"><span class="pre">ChatGooglePalm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback_manager</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">client</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'models/chat-bison-001'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">google_api_key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/langchain/chat_models/google_palm.html#ChatGooglePalm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain.chat_models.ChatGooglePalm" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">langchain.chat_models.base.BaseChatModel</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pydantic.main.BaseModel</span></code></p>
<p>Wrapper around Google’s PaLM Chat API.</p>
<p>To use you must have the google.generativeai Python package installed and
either:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">GOOGLE_API_KEY`</span></code> environment varaible set with your API key, or</p></li>
<li><p>Pass your API key using the google_api_key kwarg to the ChatGoogle
constructor.</p></li>
</ol>
</div></blockquote>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatGooglePalm</span>
<span class="n">chat</span> <span class="o">=</span> <span class="n">ChatGooglePalm</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>verbose</strong> (<em>bool</em>) – </p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>langchain.callbacks.base.BaseCallbackHandler</em><em>]</em><em>, </em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>callback_manager</strong> (<em>Optional</em><em>[</em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em>) – </p></li>
<li><p><strong>client</strong> (<em>Any</em>) – </p></li>
<li><p><strong>model_name</strong> (<em>str</em>) – </p></li>
<li><p><strong>google_api_key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>temperature</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – </p></li>
<li><p><strong>top_p</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – </p></li>
<li><p><strong>top_k</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – </p></li>
<li><p><strong>n</strong> (<em>int</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatGooglePalm.google_api_key">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">google_api_key</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.ChatGooglePalm.google_api_key" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatGooglePalm.model_name">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'models/chat-bison-001'</span></em><a class="headerlink" href="#langchain.chat_models.ChatGooglePalm.model_name" title="Permalink to this definition"></a></dt>
<dd><p>Model name to use.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatGooglePalm.n">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1</span></em><a class="headerlink" href="#langchain.chat_models.ChatGooglePalm.n" title="Permalink to this definition"></a></dt>
<dd><p>Number of chat completions to generate for each prompt. Note that the API may
not return the full n completions if duplicates are generated.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatGooglePalm.temperature">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">temperature</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.ChatGooglePalm.temperature" title="Permalink to this definition"></a></dt>
<dd><p>Run inference with this temperature. Must by in the closed
interval [0.0, 1.0].</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatGooglePalm.top_k">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">top_k</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.ChatGooglePalm.top_k" title="Permalink to this definition"></a></dt>
<dd><p>Decode using top-k sampling: consider the set of top_k most probable tokens.
Must be positive.</p>
</dd></dl>

<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatGooglePalm.top_p">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">top_p</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain.chat_models.ChatGooglePalm.top_p" title="Permalink to this definition"></a></dt>
<dd><p>Decode using nucleus sampling: consider the smallest set of tokens whose
probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].</p>
</dd></dl>

</dd></dl>

<dl class="py class pydantic_model">
<dt class="sig sig-object py" id="langchain.chat_models.ChatVertexAI">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">langchain.chat_models.</span></span><span class="sig-name descname"><span class="pre">ChatVertexAI</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback_manager</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">client</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'chat-bison'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_output_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.95</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">project</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">location</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'us-central1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">credentials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/langchain/chat_models/vertexai.html#ChatVertexAI"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain.chat_models.ChatVertexAI" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">langchain.llms.vertexai._VertexAICommon</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">langchain.chat_models.base.BaseChatModel</span></code></p>
<p>Wrapper around Vertex AI large language models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>verbose</strong> (<em>bool</em>) – </p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>langchain.callbacks.base.BaseCallbackHandler</em><em>]</em><em>, </em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>callback_manager</strong> (<em>Optional</em><em>[</em><em>langchain.callbacks.base.BaseCallbackManager</em><em>]</em>) – </p></li>
<li><p><strong>client</strong> (<em>_LanguageModel</em>) – </p></li>
<li><p><strong>model_name</strong> (<em>str</em>) – </p></li>
<li><p><strong>temperature</strong> (<em>float</em>) – </p></li>
<li><p><strong>max_output_tokens</strong> (<em>int</em>) – </p></li>
<li><p><strong>top_p</strong> (<em>float</em>) – </p></li>
<li><p><strong>top_k</strong> (<em>int</em>) – </p></li>
<li><p><strong>project</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – </p></li>
<li><p><strong>location</strong> (<em>str</em>) – </p></li>
<li><p><strong>credentials</strong> (<em>Any</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain.chat_models.ChatVertexAI.model_name">
<em class="property"><span class="pre">attribute</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'chat-bison'</span></em><a class="headerlink" href="#langchain.chat_models.ChatVertexAI.model_name" title="Permalink to this definition"></a></dt>
<dd><p>Model name to use.</p>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="llms.html" class="btn btn-neutral float-left" title="LLMs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="output_parsers.html" class="btn btn-neutral float-right" title="Output Parsers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Harrison Chase.
      <span class="lastupdated">Last updated on Jun 14, 2023.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>