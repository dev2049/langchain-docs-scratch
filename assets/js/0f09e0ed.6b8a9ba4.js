"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[65832],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>g});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var i=a.createContext({}),p=function(e){var t=a.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},m=function(e){var t=p(e.components);return a.createElement(i.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,i=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),c=p(n),u=o,g=c["".concat(i,".").concat(u)]||c[u]||d[u]||r;return n?a.createElement(g,s(s({ref:t},m),{},{components:n})):a.createElement(g,s({ref:t},m))}));function g(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,s=new Array(r);s[0]=u;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l[c]="string"==typeof e?e:o,s[1]=l;for(var p=2;p<r;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},96899:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>c,default:()=>f,frontMatter:()=>m,metadata:()=>d,toc:()=>g});var a=n(87462),o=(n(67294),n(3905));const r=(s="CodeOutputBlock",function(e){return console.warn("Component "+s+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var s;const l={toc:[]},i="wrapper";function p(e){let{components:t,...n}=e;return(0,o.kt)(i,(0,a.Z)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nchat = ChatOpenAI(temperature=0)\n")),(0,o.kt)("p",null,"You can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are ",(0,o.kt)("inlineCode",{parentName:"p"},"AIMessage"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"HumanMessage"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"SystemMessage"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatMessage")," -- ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatMessage")," takes in an arbitrary role parameter. Most of the time, you'll just be dealing with ",(0,o.kt)("inlineCode",{parentName:"p"},"HumanMessage"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"AIMessage"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"SystemMessage")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'chat([HumanMessage(content="Translate this sentence from English to French. I love programming.")])\n')),(0,o.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'    AIMessage(content="J\'aime programmer.", additional_kwargs={})\n'))),(0,o.kt)("p",null,"OpenAI's chat model supports multiple messages as input. See ",(0,o.kt)("a",{parentName:"p",href:"https://platform.openai.com/docs/guides/chat/chat-vs-completions"},"here")," for more information. Here is an example of sending a system and user message to the chat model:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'messages = [\n    SystemMessage(content="You are a helpful assistant that translates English to French."),\n    HumanMessage(content="I love programming.")\n]\nchat(messages)\n')),(0,o.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'    AIMessage(content="J\'aime programmer.", additional_kwargs={})\n'))),(0,o.kt)("p",null,"You can go one step further and generate completions for multiple sets of messages using ",(0,o.kt)("inlineCode",{parentName:"p"},"generate"),". This returns an ",(0,o.kt)("inlineCode",{parentName:"p"},"LLMResult")," with an additional ",(0,o.kt)("inlineCode",{parentName:"p"},"message")," parameter."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'batch_messages = [\n    [\n        SystemMessage(content="You are a helpful assistant that translates English to French."),\n        HumanMessage(content="I love programming.")\n    ],\n    [\n        SystemMessage(content="You are a helpful assistant that translates English to French."),\n        HumanMessage(content="I love artificial intelligence.")\n    ],\n]\nresult = chat.generate(batch_messages)\nresult\n')),(0,o.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}})\n"))),(0,o.kt)("p",null,"You can recover things like token usage from this LLMResult"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"result.llm_output\n")),(0,o.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    {'token_usage': {'prompt_tokens': 57,\n      'completion_tokens': 20,\n      'total_tokens': 77}}\n"))))}p.isMDXComponent=!0;const m={sidebar_position:1},c="Chat models",d={unversionedId:"modules/model_io/models/chat/index",id:"modules/model_io/models/chat/index",title:"Chat models",description:"Chat models are a variation on language models.",source:"@site/docs/modules/model_io/models/chat/index.mdx",sourceDirName:"modules/model_io/models/chat",slug:"/modules/model_io/models/chat/",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/chat/",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/model_io/models/chat/index.mdx",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"sidebar",previous:{title:"Writer",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/writer"},next:{title:"LLMChain",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/chat/how_to/llm_chain"}},u={},g=[{value:"Get started",id:"get-started",level:2}],h={toc:g},k="wrapper";function f(e){let{components:t,...n}=e;return(0,o.kt)(k,(0,a.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"chat-models"},"Chat models"),(0,o.kt)("p",null,'Chat models are a variation on language models.\nWhile chat models use language models under the hood, the interface they expose is a bit different.\nRather than expose a "text in, text out" API, they expose an interface where "chat messages" are the inputs and outputs.'),(0,o.kt)("p",null,"Chat model APIs are fairly new, so we are still figuring out the correct abstractions."),(0,o.kt)("p",null,"The following sections of documentation are provided:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"How-to guides"),": Walkthroughs of core functionality, like streaming, creating chat prompts, etc.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Integrations"),": How to use different chat model providers (OpenAI, Hugging Face, etc)."))),(0,o.kt)("h2",{id:"get-started"},"Get started"),(0,o.kt)("p",null,"The chat model interface is based around messages rather than raw text."),(0,o.kt)(p,{mdxType:"GetStarted"}))}f.isMDXComponent=!0}}]);