"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[85254],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>h});var a=n(67294);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},m=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,l=e.mdxType,r=e.originalType,s=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),c=p(n),d=l,h=c["".concat(s,".").concat(d)]||c[d]||u[d]||r;return n?a.createElement(h,o(o({ref:t},m),{},{components:n})):a.createElement(h,o({ref:t},m))}));function h(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=n.length,o=new Array(r);o[0]=d;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[c]="string"==typeof e?e:l,o[1]=i;for(var p=2;p<r;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},72550:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>p});var a=n(87462),l=(n(67294),n(3905));const r={},o="Petals",i={unversionedId:"modules/model_io/models/llms/integrations/petals_example",id:"modules/model_io/models/llms/integrations/petals_example",title:"Petals",description:"Petals runs 100B+ language models at home, BitTorrent-style.",source:"@site/docs/modules/model_io/models/llms/integrations/petals_example.md",sourceDirName:"modules/model_io/models/llms/integrations",slug:"/modules/model_io/models/llms/integrations/petals_example",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/petals_example",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/model_io/models/llms/integrations/petals_example.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"OpenLM",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/openlm"},next:{title:"PipelineAI",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/pipelineai_example"}},s={},p=[{value:"Install petals",id:"install-petals",level:2},{value:"Imports",id:"imports",level:2},{value:"Set the Environment API Key",id:"set-the-environment-api-key",level:2},{value:"Create the Petals instance",id:"create-the-petals-instance",level:2},{value:"Create a Prompt Template",id:"create-a-prompt-template",level:2},{value:"Initiate the LLMChain",id:"initiate-the-llmchain",level:2},{value:"Run the LLMChain",id:"run-the-llmchain",level:2}],m=(c="CodeOutputBlock",function(e){return console.warn("Component "+c+" was not imported, exported, or provided by MDXProvider as global scope"),(0,l.kt)("div",e)});var c;const u={toc:p},d="wrapper";function h(e){let{components:t,...n}=e;return(0,l.kt)(d,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"petals"},"Petals"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"Petals")," runs 100B+ language models at home, BitTorrent-style."),(0,l.kt)("p",null,"This notebook goes over how to use Langchain with ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/bigscience-workshop/petals"},"Petals"),"."),(0,l.kt)("h2",{id:"install-petals"},"Install petals"),(0,l.kt)("p",null,"The ",(0,l.kt)("inlineCode",{parentName:"p"},"petals")," package is required to use the Petals API. Install ",(0,l.kt)("inlineCode",{parentName:"p"},"petals")," using ",(0,l.kt)("inlineCode",{parentName:"p"},"pip3 install petals"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"pip3 install petals\n")),(0,l.kt)("h2",{id:"imports"},"Imports"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import os\nfrom langchain.llms import Petals\nfrom langchain import PromptTemplate, LLMChain\n")),(0,l.kt)("h2",{id:"set-the-environment-api-key"},"Set the Environment API Key"),(0,l.kt)("p",null,"Make sure to get ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/docs/api-inference/quicktour#get-your-api-token"},"your API key")," from Huggingface."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from getpass import getpass\n\nHUGGINGFACE_API_KEY = getpass()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"     \xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'os.environ["HUGGINGFACE_API_KEY"] = HUGGINGFACE_API_KEY\n')),(0,l.kt)("h2",{id:"create-the-petals-instance"},"Create the Petals instance"),(0,l.kt)("p",null,"You can specify different parameters such as the model name, max new tokens, temperature, etc."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# this can take several minutes to download big files!\n\nllm = Petals(model_name="bigscience/bloom-petals")\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    Downloading:   1%|\u258f                        | 40.8M/7.19G [00:24<15:44, 7.57MB/s]\n"))),(0,l.kt)("h2",{id:"create-a-prompt-template"},"Create a Prompt Template"),(0,l.kt)("p",null,"We will create a prompt template for Question and Answer."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'template = """Question: {question}\n\nAnswer: Let\'s think step by step."""\n\nprompt = PromptTemplate(template=template, input_variables=["question"])\n')),(0,l.kt)("h2",{id:"initiate-the-llmchain"},"Initiate the LLMChain"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"llm_chain = LLMChain(prompt=prompt, llm=llm)\n")),(0,l.kt)("h2",{id:"run-the-llmchain"},"Run the LLMChain"),(0,l.kt)("p",null,"Provide a question and run the LLMChain."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"\n\nllm_chain.run(question)\n')))}h.isMDXComponent=!0}}]);