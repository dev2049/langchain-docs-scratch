"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[40243],{3905:(e,n,o)=>{o.d(n,{Zo:()=>c,kt:()=>y});var t=o(67294);function r(e,n,o){return n in e?Object.defineProperty(e,n,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[n]=o,e}function a(e,n){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),o.push.apply(o,t)}return o}function l(e){for(var n=1;n<arguments.length;n++){var o=null!=arguments[n]?arguments[n]:{};n%2?a(Object(o),!0).forEach((function(n){r(e,n,o[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):a(Object(o)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(o,n))}))}return e}function s(e,n){if(null==e)return{};var o,t,r=function(e,n){if(null==e)return{};var o,t,r={},a=Object.keys(e);for(t=0;t<a.length;t++)o=a[t],n.indexOf(o)>=0||(r[o]=e[o]);return r}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(t=0;t<a.length;t++)o=a[t],n.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(r[o]=e[o])}return r}var i=t.createContext({}),u=function(e){var n=t.useContext(i),o=n;return e&&(o="function"==typeof e?e(n):l(l({},n),e)),o},c=function(e){var n=u(e.components);return t.createElement(i.Provider,{value:n},e.children)},m="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},d=t.forwardRef((function(e,n){var o=e.components,r=e.mdxType,a=e.originalType,i=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=u(o),d=r,y=m["".concat(i,".").concat(d)]||m[d]||p[d]||a;return o?t.createElement(y,l(l({ref:n},c),{},{components:o})):t.createElement(y,l({ref:n},c))}));function y(e,n){var o=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var a=o.length,l=new Array(a);l[0]=d;var s={};for(var i in n)hasOwnProperty.call(n,i)&&(s[i]=n[i]);s.originalType=e,s[m]="string"==typeof e?e:r,l[1]=s;for(var u=2;u<a;u++)l[u]=o[u];return t.createElement.apply(null,l)}return t.createElement.apply(null,o)}d.displayName="MDXCreateElement"},70221:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>i,contentTitle:()=>l,default:()=>y,frontMatter:()=>a,metadata:()=>s,toc:()=>u});var t=o(87462),r=(o(67294),o(3905));const a={},l="Async API",s={unversionedId:"modules/model_io/models/llms/how_to/async_llm",id:"modules/model_io/models/llms/how_to/async_llm",title:"Async API",description:"LangChain provides async support for LLMs by leveraging the asyncio library.",source:"@site/docs/modules/model_io/models/llms/how_to/async_llm.md",sourceDirName:"modules/model_io/models/llms/how_to",slug:"/modules/model_io/models/llms/how_to/async_llm",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/how_to/async_llm",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/model_io/models/llms/how_to/async_llm.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"LLMs",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/"},next:{title:"Custom LLM",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/how_to/custom_llm"}},i={},u=[],c=(m="CodeOutputBlock",function(e){return console.warn("Component "+m+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var m;const p={toc:u},d="wrapper";function y(e){let{components:n,...o}=e;return(0,r.kt)(d,(0,t.Z)({},p,o,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"async-api"},"Async API"),(0,r.kt)("p",null,"LangChain provides async support for LLMs by leveraging the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.python.org/3/library/asyncio.html"},"asyncio")," library."),(0,r.kt)("p",null,"Async support is particularly useful for calling multiple LLMs concurrently, as these calls are network-bound. Currently, ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenAI"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"PromptLayerOpenAI"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"ChatOpenAI")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"Anthropic")," are supported, but async support for other LLMs is on the roadmap."),(0,r.kt)("p",null,"You can use the ",(0,r.kt)("inlineCode",{parentName:"p"},"agenerate")," method to call an OpenAI LLM asynchronously."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import time\nimport asyncio\n\nfrom langchain.llms import OpenAI\n\ndef generate_serially():\n    llm = OpenAI(temperature=0.9)\n    for _ in range(10):\n        resp = llm.generate([\"Hello, how are you?\"])\n        print(resp.generations[0][0].text)\n\n\nasync def async_generate(llm):\n    resp = await llm.agenerate([\"Hello, how are you?\"])\n    print(resp.generations[0][0].text)\n\n\nasync def generate_concurrently():\n    llm = OpenAI(temperature=0.9)\n    tasks = [async_generate(llm) for _ in range(10)]\n    await asyncio.gather(*tasks)\n\n\ns = time.perf_counter()\n# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\nawait generate_concurrently() \nelapsed = time.perf_counter() - s\nprint('\\033[1m' + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + '\\033[0m')\n\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint('\\033[1m' + f\"Serial executed in {elapsed:0.2f} seconds.\" + '\\033[0m')\n")),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    \n    \n    I'm doing well, thank you. How about you?\n    \n    \n    I'm doing well, thank you. How about you?\n    \n    \n    I'm doing well, how about you?\n    \n    \n    I'm doing well, thank you. How about you?\n    \n    \n    I'm doing well, thank you. How about you?\n    \n    \n    I'm doing well, thank you. How about yourself?\n    \n    \n    I'm doing well, thank you! How about you?\n    \n    \n    I'm doing well, thank you. How about you?\n    \n    \n    I'm doing well, thank you! How about you?\n    \n    \n    I'm doing well, thank you. How about you?\n    Concurrent executed in 1.39 seconds.\n    \n    \n    I'm doing well, thank you. How about you?\n    \n    \n    I'm doing well, thank you. How about you?\n    \n    I'm doing well, thank you. How about you?\n    \n    \n    I'm doing well, thank you. How about you?\n    \n    \n    I'm doing well, thank you. How about yourself?\n    \n    \n    I'm doing well, thanks for asking. How about you?\n    \n    \n    I'm doing well, thanks! How about you?\n    \n    \n    I'm doing well, thank you. How about you?\n    \n    \n    I'm doing well, thank you. How about yourself?\n    \n    \n    I'm doing well, thanks for asking. How about you?\n    Serial executed in 5.77 seconds.\n"))))}y.isMDXComponent=!0}}]);