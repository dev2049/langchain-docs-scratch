"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[88024],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>g});var r=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,r,o=function(e,n){if(null==e)return{};var t,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var l=r.createContext({}),p=function(e){var n=r.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},c=function(e){var n=p(e.components);return r.createElement(l.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef((function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),m=p(t),d=o,g=m["".concat(l,".").concat(d)]||m[d]||u[d]||a;return t?r.createElement(g,s(s({ref:n},c),{},{components:t})):r.createElement(g,s({ref:n},c))}));function g(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,s=new Array(a);s[0]=d;var i={};for(var l in n)hasOwnProperty.call(n,l)&&(i[l]=n[l]);i.originalType=e,i[m]="string"==typeof e?e:o,s[1]=i;for(var p=2;p<a;p++)s[p]=t[p];return r.createElement.apply(null,s)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},94058:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>g,frontMatter:()=>a,metadata:()=>i,toc:()=>p});var r=t(87462),o=(t(67294),t(3905));const a={},s="JSONFormer",i={unversionedId:"modules/model_io/models/llms/integrations/jsonformer_experimental",id:"modules/model_io/models/llms/integrations/jsonformer_experimental",title:"JSONFormer",description:"JSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.",source:"@site/docs/modules/model_io/models/llms/integrations/jsonformer_experimental.md",sourceDirName:"modules/model_io/models/llms/integrations",slug:"/modules/model_io/models/llms/integrations/jsonformer_experimental",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/jsonformer_experimental",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/model_io/models/llms/integrations/jsonformer_experimental.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Huggingface TextGen Inference",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/huggingface_textgen_inference"},next:{title:"Llama-cpp",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/llamacpp"}},l={},p=[{value:"HuggingFace Baseline",id:"huggingface-baseline",level:3},{value:"JSONFormer LLM Wrapper",id:"jsonformer-llm-wrapper",level:2}],c=(m="CodeOutputBlock",function(e){return console.warn("Component "+m+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var m;const u={toc:p},d="wrapper";function g(e){let{components:n,...t}=e;return(0,o.kt)(d,(0,r.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"jsonformer"},"JSONFormer"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/1rgs/jsonformer"},"JSONFormer")," is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema."),(0,o.kt)("p",null,"It works by filling in the structure tokens and then sampling the content tokens from the model."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Warning - this module is still experimental")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install --upgrade jsonformer > /dev/null\n")),(0,o.kt)("h3",{id:"huggingface-baseline"},"HuggingFace Baseline"),(0,o.kt)("p",null,"First, let's establish a qualitative baseline by checking the output of the model without structured decoding."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import logging\nlogging.basicConfig(level=logging.ERROR)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from typing import Optional\nfrom langchain.tools import tool\nimport os\nimport json\nimport requests\n\nHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")\n\n@tool\ndef ask_star_coder(query: str, \n                   temperature: float = 1.0,\n                   max_new_tokens: float = 250):\n    """Query the BigCode StarCoder model about coding questions."""\n    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"\n    headers = {\n        "Authorization": f"Bearer {HF_TOKEN}",\n        "content-type": "application/json"\n            }\n    payload = {\n        "inputs": f"{query}\\n\\nAnswer:",\n        "temperature": temperature,\n        "max_new_tokens": int(max_new_tokens),\n    }\n    response = requests.post(url, headers=headers, data=json.dumps(payload))\n    response.raise_for_status()\n    return json.loads(response.content.decode("utf-8"))\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'prompt = """You must respond using JSON format, with a single action and single action input.\nYou may \'ask_star_coder\' for help on coding problems.\n\n{arg_schema}\n\nEXAMPLES\n----\nHuman: "So what\'s all this about a GIL?"\nAI Assistant:{{\n  "action": "ask_star_coder",\n  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"\n}}\nObservation: "The GIL is python\'s Global Interpreter Lock"\nHuman: "Could you please write a calculator program in LISP?"\nAI Assistant:{{\n  "action": "ask_star_coder",\n  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}\n}}\nObservation: "(defun add (x y) (+ x y))\\n(defun sub (x y) (- x y ))"\nHuman: "What\'s the difference between an SVM and an LLM?"\nAI Assistant:{{\n  "action": "ask_star_coder",\n  "action_input": {{"query": "What\'s the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}\n}}\nObservation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."\n\nBEGIN! Answer the Human\'s question as best as you are able.\n------\nHuman: \'What\'s the difference between an iterator and an iterable?\'\nAI Assistant:""".format(arg_schema=ask_star_coder.args)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from transformers import pipeline\nfrom langchain.llms import HuggingFacePipeline\n\nhf_model = pipeline("text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)\n\noriginal_model = HuggingFacePipeline(pipeline=hf_model)\n\ngenerated = original_model.predict(prompt, stop=["Observation:", "Human:"])\nprint(generated)\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n     'What's the difference between an iterator and an iterable?'\n    \n"))),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},(0,o.kt)("em",{parentName:"strong"},"That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder."))),(0,o.kt)("h2",{id:"jsonformer-llm-wrapper"},"JSONFormer LLM Wrapper"),(0,o.kt)("p",null,"Let's try that again, now providing a the Action input's JSON Schema to the model."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'decoder_schema = {\n    "title": "Decoding Schema",\n    "type": "object",\n    "properties": {\n        "action": {"type": "string", "default": ask_star_coder.name},\n        "action_input": {\n            "type": "object",\n            "properties": ask_star_coder.args,\n        }\n    }\n}\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.experimental.llms import JsonFormer\njson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'results = json_former.predict(prompt, stop=["Observation:", "Human:"])\nprint(results)\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'    {"action": "ask_star_coder", "action_input": {"query": "What\'s the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}\n'))),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Voila! Free of parsing errors.")))}g.isMDXComponent=!0}}]);