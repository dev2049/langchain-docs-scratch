"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[71844],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,l=e.originalType,p=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),c=s(n),m=o,h=c["".concat(p,".").concat(m)]||c[m]||d[m]||l;return n?a.createElement(h,r(r({ref:t},u),{},{components:n})):a.createElement(h,r({ref:t},u))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var l=n.length,r=new Array(l);r[0]=m;var i={};for(var p in t)hasOwnProperty.call(t,p)&&(i[p]=t[p]);i.originalType=e,i[c]="string"==typeof e?e:o,r[1]=i;for(var s=2;s<l;s++)r[s]=n[s];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},90596:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>m,contentTitle:()=>c,default:()=>f,frontMatter:()=>u,metadata:()=>d,toc:()=>h});var a=n(87462),o=(n(67294),n(3905));const l=(r="CodeOutputBlock",function(e){return console.warn("Component "+r+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var r;const i={toc:[{value:"Additional ways of running LLM Chain",id:"additional-ways-of-running-llm-chain",level:2},{value:"Parsing the outputs",id:"parsing-the-outputs",level:2},{value:"Initialize from string",id:"initialize-from-string",level:2}]},p="wrapper";function s(e){let{components:t,...n}=e;return(0,o.kt)(p,(0,a.Z)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain import PromptTemplate, OpenAI, LLMChain\n\nprompt_template = "What is a good name for a company that makes {product}?"\n\nllm = OpenAI(temperature=0)\nllm_chain = LLMChain(\n    llm=llm,\n    prompt=PromptTemplate.from_template(prompt_template)\n)\nllm_chain("colorful socks")\n')),(0,o.kt)(l,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    {'product': 'colorful socks', 'text': '\\n\\nSocktastic!'}\n"))),(0,o.kt)("h2",{id:"additional-ways-of-running-llm-chain"},"Additional ways of running LLM Chain"),(0,o.kt)("p",null,"Aside from ",(0,o.kt)("inlineCode",{parentName:"p"},"__call__")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"run")," methods shared by all ",(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," object, ",(0,o.kt)("inlineCode",{parentName:"p"},"LLMChain")," offers a few more ways of calling the chain logic:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"apply")," allows you run the chain against a list of inputs:")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'input_list = [\n    {"product": "socks"},\n    {"product": "computer"},\n    {"product": "shoes"}\n]\n\nllm_chain.apply(input_list)\n')),(0,o.kt)(l,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    [{'text': '\\n\\nSocktastic!'},\n     {'text': '\\n\\nTechCore Solutions.'},\n     {'text': '\\n\\nFootwear Factory.'}]\n"))),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"generate")," is similar to ",(0,o.kt)("inlineCode",{parentName:"li"},"apply"),", except it return an ",(0,o.kt)("inlineCode",{parentName:"li"},"LLMResult")," instead of string. ",(0,o.kt)("inlineCode",{parentName:"li"},"LLMResult")," often contains useful generation such as token usages and finish reason.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"llm_chain.generate(input_list)\n")),(0,o.kt)(l,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    LLMResult(generations=[[Generation(text='\\n\\nSocktastic!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nTechCore Solutions.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nFootwear Factory.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 36, 'total_tokens': 55, 'completion_tokens': 19}, 'model_name': 'text-davinci-003'})\n"))),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"predict")," is similar to ",(0,o.kt)("inlineCode",{parentName:"li"},"run")," method except that the input keys are specified as keyword arguments instead of a Python dict.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# Single input example\nllm_chain.predict(product="colorful socks")\n')),(0,o.kt)(l,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    '\\n\\nSocktastic!'\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# Multiple inputs example\n\ntemplate = """Tell me a {adjective} joke about {subject}."""\nprompt = PromptTemplate(template=template, input_variables=["adjective", "subject"])\nllm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0))\n\nllm_chain.predict(adjective="sad", subject="ducks")\n')),(0,o.kt)(l,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    '\\n\\nQ: What did the duck say when his friend died?\\nA: Quack, quack, goodbye.'\n"))),(0,o.kt)("h2",{id:"parsing-the-outputs"},"Parsing the outputs"),(0,o.kt)("p",null,"By default, ",(0,o.kt)("inlineCode",{parentName:"p"},"LLMChain")," does not parse the output even if the underlying ",(0,o.kt)("inlineCode",{parentName:"p"},"prompt")," object has an output parser. If you would like to apply that output parser on the LLM output, use ",(0,o.kt)("inlineCode",{parentName:"p"},"predict_and_parse")," instead of ",(0,o.kt)("inlineCode",{parentName:"p"},"predict")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"apply_and_parse")," instead of ",(0,o.kt)("inlineCode",{parentName:"p"},"apply"),". "),(0,o.kt)("p",null,"With ",(0,o.kt)("inlineCode",{parentName:"p"},"predict"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.output_parsers import CommaSeparatedListOutputParser\n\noutput_parser = CommaSeparatedListOutputParser()\ntemplate = """List all the colors in a rainbow"""\nprompt = PromptTemplate(template=template, input_variables=[], output_parser=output_parser)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nllm_chain.predict()\n')),(0,o.kt)(l,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    '\\n\\nRed, orange, yellow, green, blue, indigo, violet'\n"))),(0,o.kt)("p",null,"With ",(0,o.kt)("inlineCode",{parentName:"p"},"predict_and_parser"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"llm_chain.predict_and_parse()\n")),(0,o.kt)(l,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    ['Red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet']\n"))),(0,o.kt)("h2",{id:"initialize-from-string"},"Initialize from string"),(0,o.kt)("p",null,"You can also construct an LLMChain from a string template directly."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'template = """Tell me a {adjective} joke about {subject}."""\nllm_chain = LLMChain.from_string(llm=llm, template=template)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'llm_chain.predict(adjective="sad", subject="ducks")\n')),(0,o.kt)(l,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    '\\n\\nQ: What did the duck say when his friend died?\\nA: Quack, quack, goodbye.'\n"))))}s.isMDXComponent=!0;const u={},c="LLM",d={unversionedId:"modules/chains/foundational/llm_chain",id:"modules/chains/foundational/llm_chain",title:"LLM",description:"An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.",source:"@site/docs/modules/chains/foundational/llm_chain.mdx",sourceDirName:"modules/chains/foundational",slug:"/modules/chains/foundational/llm_chain",permalink:"/langchain-docs-scratch/docs/modules/chains/foundational/llm_chain",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/chains/foundational/llm_chain.mdx",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Foundational",permalink:"/langchain-docs-scratch/docs/modules/chains/foundational/"},next:{title:"Router",permalink:"/langchain-docs-scratch/docs/modules/chains/foundational/router"}},m={},h=[{value:"Get started",id:"get-started",level:2}],k={toc:h},g="wrapper";function f(e){let{components:t,...n}=e;return(0,o.kt)(g,(0,a.Z)({},k,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"llm"},"LLM"),(0,o.kt)("p",null,"An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents."),(0,o.kt)("p",null,"An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output."),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://drive.google.com/uc?id=1g4Cv0GbIxSk5xyIDSAL7BIu-hOT7KMNT",alt:null})),(0,o.kt)("h2",{id:"get-started"},"Get started"),(0,o.kt)(s,{mdxType:"Example"}))}f.isMDXComponent=!0}}]);