"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[65671],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>y});var o=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var m=o.createContext({}),l=function(e){var t=o.useContext(m),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},u=function(e){var t=l(e.components);return o.createElement(m.Provider,{value:t},e.children)},p="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,m=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),p=l(n),h=r,y=p["".concat(m,".").concat(h)]||p[h]||c[h]||a;return n?o.createElement(y,s(s({ref:t},u),{},{components:n})):o.createElement(y,s({ref:t},u))}));function y(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,s=new Array(a);s[0]=h;var i={};for(var m in t)hasOwnProperty.call(t,m)&&(i[m]=t[m]);i.originalType=e,i[p]="string"==typeof e?e:r,s[1]=i;for(var l=2;l<a;l++)s[l]=n[l];return o.createElement.apply(null,s)}return o.createElement.apply(null,n)}h.displayName="MDXCreateElement"},46309:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>p,default:()=>f,frontMatter:()=>u,metadata:()=>c,toc:()=>y});var o=n(87462),r=(n(67294),n(3905));const a=(s="CodeOutputBlock",function(e){return console.warn("Component "+s+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var s;const i={toc:[{value:"Initializing with messages",id:"initializing-with-messages",level:2},{value:"Using in a chain",id:"using-in-a-chain",level:2}]},m="wrapper";function l(e){let{components:t,...n}=e;return(0,r.kt)(m,(0,o.Z)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\nfrom langchain.llms import OpenAI\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\nmemory.save_context({"input": "hi"}, {"output": "whats up"})\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"memory.load_memory_variables({})\n")),(0,r.kt)(a,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    {'history': '\\nThe human greets the AI, to which the AI responds.'}\n"))),(0,r.kt)("p",null,"We can also get the history as a list of messages (this is useful if you are using this with a chat model)."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'memory = ConversationSummaryMemory(llm=OpenAI(temperature=0), return_messages=True)\nmemory.save_context({"input": "hi"}, {"output": "whats up"})\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"memory.load_memory_variables({})\n")),(0,r.kt)(a,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    {'history': [SystemMessage(content='\\nThe human greets the AI, to which the AI responds.', additional_kwargs={})]}\n"))),(0,r.kt)("p",null,"We can also utilize the ",(0,r.kt)("inlineCode",{parentName:"p"},"predict_new_summary")," method directly."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'messages = memory.chat_memory.messages\nprevious_summary = ""\nmemory.predict_new_summary(messages, previous_summary)\n')),(0,r.kt)(a,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    '\\nThe human greets the AI, to which the AI responds.'\n"))),(0,r.kt)("h2",{id:"initializing-with-messages"},"Initializing with messages"),(0,r.kt)("p",null,"If you have messages outside this class, you can easily initialize the class with ChatMessageHistory. During loading, a summary will be calculated."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'history = ChatMessageHistory()\nhistory.add_user_message("hi")\nhistory.add_ai_message("hi there!")\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"memory = ConversationSummaryMemory.from_messages(llm=OpenAI(temperature=0), chat_memory=history, return_messages=True)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"memory.buffer\n")),(0,r.kt)(a,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    '\\nThe human greets the AI, to which the AI responds with a friendly greeting.'\n"))),(0,r.kt)("h2",{id:"using-in-a-chain"},"Using in a chain"),(0,r.kt)("p",null,"Let's walk through an example of using this in a chain, again setting ",(0,r.kt)("inlineCode",{parentName:"p"},"verbose=True")," so we can see the prompt."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.llms import OpenAI\nfrom langchain.chains import ConversationChain\nllm = OpenAI(temperature=0)\nconversation_with_summary = ConversationChain(\n    llm=llm, \n    memory=ConversationSummaryMemory(llm=OpenAI()),\n    verbose=True\n)\nconversation_with_summary.predict(input="Hi, what\'s up?")\n')),(0,r.kt)(a,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    \n    \n    > Entering new ConversationChain chain...\n    Prompt after formatting:\n    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n    \n    Current conversation:\n    \n    Human: Hi, what's up?\n    AI:\n    \n    > Finished chain.\n\n\n\n\n\n    \" Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\"\n"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'conversation_with_summary.predict(input="Tell me more about it!")\n')),(0,r.kt)(a,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    \n    \n    > Entering new ConversationChain chain...\n    Prompt after formatting:\n    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n    \n    Current conversation:\n    \n    The human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue.\n    Human: Tell me more about it!\n    AI:\n    \n    > Finished chain.\n\n\n\n\n\n    \" Sure! The customer is having trouble with their computer not connecting to the internet. I'm helping them troubleshoot the issue and figure out what the problem is. So far, we've tried resetting the router and checking the network settings, but the issue still persists. We're currently looking into other possible solutions.\"\n"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'conversation_with_summary.predict(input="Very cool -- what is the scope of the project?")\n')),(0,r.kt)(a,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'    \n    \n    > Entering new ConversationChain chain...\n    Prompt after formatting:\n    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n    \n    Current conversation:\n    \n    The human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue where their computer was not connecting to the internet. The AI was troubleshooting the issue and had already tried resetting the router and checking the network settings, but the issue still persisted and they were looking into other possible solutions.\n    Human: Very cool -- what is the scope of the project?\n    AI:\n    \n    > Finished chain.\n\n\n\n\n\n    " The scope of the project is to troubleshoot the customer\'s computer issue and find a solution that will allow them to connect to the internet. We are currently exploring different possibilities and have already tried resetting the router and checking the network settings, but the issue still persists."\n'))))}l.isMDXComponent=!0;const u={},p="Conversation summary memory",c={unversionedId:"modules/memory/how_to/summary",id:"modules/memory/how_to/summary",title:"Conversation summary memory",description:"Now let's take a look at using a slightly more complex type of memory - ConversationSummaryMemory. This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time.",source:"@site/docs/modules/memory/how_to/summary.mdx",sourceDirName:"modules/memory/how_to",slug:"/modules/memory/how_to/summary",permalink:"/langchain-docs-scratch/docs/modules/memory/how_to/summary",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/memory/how_to/summary.mdx",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"How to use multiple memory classes in the same chain",permalink:"/langchain-docs-scratch/docs/modules/memory/how_to/multiple_memory"},next:{title:"ConversationSummaryBufferMemory",permalink:"/langchain-docs-scratch/docs/modules/memory/how_to/summary_buffer"}},h={},y=[],d={toc:y},g="wrapper";function f(e){let{components:t,...n}=e;return(0,r.kt)(g,(0,o.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"conversation-summary-memory"},"Conversation summary memory"),(0,r.kt)("p",null,"Now let's take a look at using a slightly more complex type of memory - ",(0,r.kt)("inlineCode",{parentName:"p"},"ConversationSummaryMemory"),". This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time.\nConversation summary memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."),(0,r.kt)("p",null,"Let's first explore the basic functionality of this type of memory."),(0,r.kt)(l,{mdxType:"Example"}))}f.isMDXComponent=!0}}]);