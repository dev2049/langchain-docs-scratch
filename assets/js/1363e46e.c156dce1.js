"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[95954],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>f});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),m=o,f=u["".concat(l,".").concat(m)]||u[m]||d[m]||r;return n?a.createElement(f,i(i({ref:t},c),{},{components:n})):a.createElement(f,i({ref:t},c))}));function f(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:o,i[1]=s;for(var p=2;p<r;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},47077:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>f,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var a=n(87462),o=(n(67294),n(3905));const r={},i="Apify",s={unversionedId:"modules/agents/tools/integrations/apify",id:"modules/agents/tools/integrations/apify",title:"Apify",description:"This notebook shows how to use the Apify integration for LangChain.",source:"@site/docs/modules/agents/tools/integrations/apify.md",sourceDirName:"modules/agents/tools/integrations",slug:"/modules/agents/tools/integrations/apify",permalink:"/langchain-docs-scratch/docs/modules/agents/tools/integrations/apify",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/agents/tools/integrations/apify.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Tool Input Schema",permalink:"/langchain-docs-scratch/docs/modules/agents/tools/how_to/tool_input_validation"},next:{title:"ArXiv API Tool",permalink:"/langchain-docs-scratch/docs/modules/agents/tools/integrations/arxiv"}},l={},p=[],c=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var u;const d={toc:p},m="wrapper";function f(e){let{components:t,...n}=e;return(0,o.kt)(m,(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"apify"},"Apify"),(0,o.kt)("p",null,"This notebook shows how to use the ",(0,o.kt)("a",{parentName:"p",href:"../../../../ecosystem/apify.md"},"Apify integration")," for LangChain."),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://apify.com"},"Apify")," is a cloud platform for web scraping and data extraction,\nwhich provides an ",(0,o.kt)("a",{parentName:"p",href:"https://apify.com/store"},"ecosystem")," of more than a thousand\nready-made apps called ",(0,o.kt)("em",{parentName:"p"},"Actors")," for various web scraping, crawling, and data extraction use cases.\nFor example, you can use it to extract Google Search results, Instagram and Facebook profiles, products from Amazon or Shopify, Google Maps reviews, etc. etc."),(0,o.kt)("p",null,"In this example, we'll use the ",(0,o.kt)("a",{parentName:"p",href:"https://apify.com/apify/website-content-crawler"},"Website Content Crawler")," Actor,\nwhich can deeply crawl websites such as documentation, knowledge bases, help centers, or blogs,\nand extract text content from the web pages. Then we feed the documents into a vector index and answer questions from it."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"#!pip install apify-client\n")),(0,o.kt)("p",null,"First, import ",(0,o.kt)("inlineCode",{parentName:"p"},"ApifyWrapper")," into your source code:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders.base import Document\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.utilities import ApifyWrapper\n")),(0,o.kt)("p",null,"Initialize it using your ",(0,o.kt)("a",{parentName:"p",href:"https://console.apify.com/account/integrations"},"Apify API token")," and for the purpose of this example, also with your OpenAI API key:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import os\nos.environ["OPENAI_API_KEY"] = "Your OpenAI API key"\nos.environ["APIFY_API_TOKEN"] = "Your Apify API token"\n\napify = ApifyWrapper()\n')),(0,o.kt)("p",null,"Then run the Actor, wait for it to finish, and fetch its results from the Apify dataset into a LangChain document loader."),(0,o.kt)("p",null,"Note that if you already have some results in an Apify dataset, you can load them directly using ",(0,o.kt)("inlineCode",{parentName:"p"},"ApifyDatasetLoader"),", as shown in ",(0,o.kt)("a",{parentName:"p",href:"../../../indexes/document_loaders/examples/apify_dataset.ipynb"},"this notebook"),". In that notebook, you'll also find the explanation of the ",(0,o.kt)("inlineCode",{parentName:"p"},"dataset_mapping_function"),", which is used to map fields from the Apify dataset records to LangChain ",(0,o.kt)("inlineCode",{parentName:"p"},"Document")," fields."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = apify.call_actor(\n    actor_id="apify/website-content-crawler",\n    run_input={"startUrls": [{"url": "https://python.langchain.com/en/latest/"}]},\n    dataset_mapping_function=lambda item: Document(\n        page_content=item["text"] or "", metadata={"source": item["url"]}\n    ),\n)\n')),(0,o.kt)("p",null,"Initialize the vector index from the crawled documents:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"index = VectorstoreIndexCreator().from_loaders([loader])\n")),(0,o.kt)("p",null,"And finally, query the vector index:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'query = "What is LangChain?"\nresult = index.query_with_sources(query)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'print(result["answer"])\nprint(result["sources"])\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"     LangChain is a standard interface through which you can interact with a variety of large language models (LLMs). It provides modules that can be used to build language model applications, and it also provides chains and agents with memory capabilities.\n    \n    https://python.langchain.com/en/latest/modules/models/llms.html, https://python.langchain.com/en/latest/getting_started/getting_started.html\n"))))}f.isMDXComponent=!0}}]);