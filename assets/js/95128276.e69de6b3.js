"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[41199],{3905:(e,t,n)=>{n.d(t,{Zo:()=>l,kt:()=>m});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=r.createContext({}),p=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},l=function(e){var t=p(e.components);return r.createElement(c.Provider,{value:t},e.children)},u="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),u=p(n),d=a,m=u["".concat(c,".").concat(d)]||u[d]||h[d]||o;return n?r.createElement(m,i(i({ref:t},l),{},{components:n})):r.createElement(m,i({ref:t},l))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=d;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[u]="string"==typeof e?e:a,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},84455:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>u,default:()=>f,frontMatter:()=>l,metadata:()=>h,toc:()=>m});var r=n(87462),a=(n(67294),n(3905));const o=(i="CodeOutputBlock",function(e){return console.warn("Component "+i+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var i;const s={toc:[{value:"Chain Type",id:"chain-type",level:2}]},c="wrapper";function p(e){let{components:t,...n}=e;return(0,a.kt)(c,(0,r.Z)({},s,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.embeddings.cohere import CohereEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\nfrom langchain.vectorstores import Chroma\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'with open("../../state_of_the_union.txt") as f:\n    state_of_the_union = f.read()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_text(state_of_the_union)\n\nembeddings = OpenAIEmbeddings()\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{"source": f"{i}-pl"} for i in range(len(texts))])\n')),(0,a.kt)(o,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    Running Chroma using direct local API.\n    Using DuckDB in-memory for database. Data will be transient.\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.chains import RetrievalQAWithSourcesChain\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from langchain import OpenAI\n\nchain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), chain_type="stuff", retriever=docsearch.as_retriever())\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'chain({"question": "What did the president say about Justice Breyer"}, return_only_outputs=True)\n')),(0,a.kt)(o,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    {'answer': ' The president honored Justice Breyer for his service and mentioned his legacy of excellence.\\n',\n     'sources': '31-pl'}\n"))),(0,a.kt)("h2",{id:"chain-type"},"Chain Type"),(0,a.kt)("p",null,"You can easily specify different chain types to load and use in the RetrievalQAWithSourcesChain chain. For a more detailed walkthrough of these types, please see ",(0,a.kt)("a",{parentName:"p",href:"qa_with_sources.ipynb"},"this notebook"),"."),(0,a.kt)("p",null,"There are two ways to load different chain types. First, you can specify the chain type argument in the ",(0,a.kt)("inlineCode",{parentName:"p"},"from_chain_type")," method. This allows you to pass in the name of the chain type you want to use. For example, in the below we change the chain type to ",(0,a.kt)("inlineCode",{parentName:"p"},"map_reduce"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'chain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), chain_type="map_reduce", retriever=docsearch.as_retriever())\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'chain({"question": "What did the president say about Justice Breyer"}, return_only_outputs=True)\n')),(0,a.kt)(o,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    {'answer': ' The president said \"Justice Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\"\\n',\n     'sources': '31-pl'}\n"))),(0,a.kt)("p",null,"The above way allows you to really simply change the chain_type, but it does provide a ton of flexibility over parameters to that chain type. If you want to control those parameters, you can load the chain directly (as you did in ",(0,a.kt)("a",{parentName:"p",href:"qa_with_sources.ipynb"},"this notebook"),") and then pass that directly to the the RetrievalQAWithSourcesChain chain with the ",(0,a.kt)("inlineCode",{parentName:"p"},"combine_documents_chain")," parameter. For example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chains.qa_with_sources import load_qa_with_sources_chain\nqa_chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type="stuff")\nqa = RetrievalQAWithSourcesChain(combine_documents_chain=qa_chain, retriever=docsearch.as_retriever())\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'qa({"question": "What did the president say about Justice Breyer"}, return_only_outputs=True)\n')),(0,a.kt)(o,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    {'answer': ' The president honored Justice Breyer for his service and mentioned his legacy of excellence.\\n',\n     'sources': '31-pl'}\n"))))}p.isMDXComponent=!0;const l={},u="Retrieval QA with sources",h={unversionedId:"modules/chains/index_examples/vector_db_qa_with_sources",id:"modules/chains/index_examples/vector_db_qa_with_sources",title:"Retrieval QA with sources",description:"This notebook goes over how to do question-answering with sources over an Index. It does this by using the RetrievalQAWithSourcesChain, which does the lookup of the documents from an Index.",source:"@site/docs/modules/chains/index_examples/vector_db_qa_with_sources.mdx",sourceDirName:"modules/chains/index_examples",slug:"/modules/chains/index_examples/vector_db_qa_with_sources",permalink:"/langchain-docs-scratch/docs/modules/chains/index_examples/vector_db_qa_with_sources",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/chains/index_examples/vector_db_qa_with_sources.mdx",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Retrieval QA",permalink:"/langchain-docs-scratch/docs/modules/chains/index_examples/vector_db_qa"},next:{title:"Vector store-augmented text generation",permalink:"/langchain-docs-scratch/docs/modules/chains/index_examples/vector_db_text_generation"}},d={},m=[],y={toc:m},_="wrapper";function f(e){let{components:t,...n}=e;return(0,a.kt)(_,(0,r.Z)({},y,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"retrieval-qa-with-sources"},"Retrieval QA with sources"),(0,a.kt)("p",null,"This notebook goes over how to do question-answering with sources over an Index. It does this by using the ",(0,a.kt)("inlineCode",{parentName:"p"},"RetrievalQAWithSourcesChain"),", which does the lookup of the documents from an Index. "),(0,a.kt)(p,{mdxType:"Example"}))}f.isMDXComponent=!0}}]);