"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[67545,27918],{85162:(e,t,n)=>{n.d(t,{Z:()=>l});var a=n(67294),o=n(86010);const r={tabItem:"tabItem_Ymn6"};function l(e){let{children:t,hidden:n,className:l}=e;return a.createElement("div",{role:"tabpanel",className:(0,o.Z)(r.tabItem,l),hidden:n},t)}},74866:(e,t,n)=>{n.d(t,{Z:()=>v});var a=n(87462),o=n(67294),r=n(86010),l=n(12466),s=n(16550),i=n(91980),p=n(67392),m=n(50012);function u(e){return function(e){return o.Children.map(e,(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:o}}=e;return{value:t,label:n,attributes:a,default:o}}))}function c(e){const{values:t,children:n}=e;return(0,o.useMemo)((()=>{const e=t??u(n);return function(e){const t=(0,p.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function h(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function d(e){let{queryString:t=!1,groupId:n}=e;const a=(0,s.k6)(),r=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,i._X)(r),(0,o.useCallback)((e=>{if(!r)return;const t=new URLSearchParams(a.location.search);t.set(r,e),a.replace({...a.location,search:t.toString()})}),[r,a])]}function g(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,r=c(e),[l,s]=(0,o.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!h({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:r}))),[i,p]=d({queryString:n,groupId:a}),[u,g]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,r]=(0,m.Nk)(n);return[a,(0,o.useCallback)((e=>{n&&r.set(e)}),[n,r])]}({groupId:a}),y=(()=>{const e=i??u;return h({value:e,tabValues:r})?e:null})();(0,o.useLayoutEffect)((()=>{y&&s(y)}),[y]);return{selectedValue:l,selectValue:(0,o.useCallback)((e=>{if(!h({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);s(e),p(e),g(e)}),[p,g,r]),tabValues:r}}var y=n(72389);const k={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function f(e){let{className:t,block:n,selectedValue:s,selectValue:i,tabValues:p}=e;const m=[],{blockElementScrollPositionUntilNextRender:u}=(0,l.o5)(),c=e=>{const t=e.currentTarget,n=m.indexOf(t),a=p[n].value;a!==s&&(u(t),i(a))},h=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=m.indexOf(e.currentTarget)+1;t=m[n]??m[0];break}case"ArrowLeft":{const n=m.indexOf(e.currentTarget)-1;t=m[n]??m[m.length-1];break}}t?.focus()};return o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":n},t)},p.map((e=>{let{value:t,label:n,attributes:l}=e;return o.createElement("li",(0,a.Z)({role:"tab",tabIndex:s===t?0:-1,"aria-selected":s===t,key:t,ref:e=>m.push(e),onKeyDown:h,onClick:c},l,{className:(0,r.Z)("tabs__item",k.tabItem,l?.className,{"tabs__item--active":s===t})}),n??t)})))}function w(e){let{lazy:t,children:n,selectedValue:a}=e;const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=r.find((e=>e.props.value===a));return e?(0,o.cloneElement)(e,{className:"margin-top--md"}):null}return o.createElement("div",{className:"margin-top--md"},r.map(((e,t)=>(0,o.cloneElement)(e,{key:t,hidden:e.props.value!==a}))))}function b(e){const t=g(e);return o.createElement("div",{className:(0,r.Z)("tabs-container",k.tabList)},o.createElement(f,(0,a.Z)({},e,t)),o.createElement(w,(0,a.Z)({},e,t)))}function v(e){const t=(0,y.Z)();return o.createElement(b,(0,a.Z)({key:String(t)},e))}},18573:(e,t,n)=>{n.d(t,{Z:()=>l});var a=n(67294),o=n(90814);function r(e){let{imports:t}=e;return a.createElement("div",{style:{paddingTop:"1.3rem",background:"var(--prism-background-color)",color:"var(--prism-color)",marginTop:"calc(-1 * var(--ifm-leading) - 5px)",marginBottom:"var(--ifm-leading)",boxShadow:"var(--ifm-global-shadow-lw)",borderBottomLeftRadius:"var(--ifm-code-border-radius)",borderBottomRightRadius:"var(--ifm-code-border-radius)"}},a.createElement("h4",{style:{paddingLeft:"0.65rem",marginBottom:"0.45rem"}},"API Reference:"),a.createElement("ul",{style:{paddingBottom:"1rem"}},t.map((e=>{let{imported:t,source:n,docs:o}=e;return a.createElement("li",null,a.createElement("a",{href:o},a.createElement("span",null,t))," ","from ",a.createElement("code",null,n))}))))}function l(e){let{children:t,...n}=e;return"string"==typeof t?a.createElement(o.Z,n,t):a.createElement(a.Fragment,null,a.createElement(o.Z,n,t.content),a.createElement(r,{imports:t.imports}))}},12729:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>j,contentTitle:()=>z,default:()=>G,frontMatter:()=>B,metadata:()=>Y,toc:()=>J});var a=n(87462),o=(n(67294),n(3905)),r=n(74866),l=n(85162),s=n(18573);const i={toc:[]},p="wrapper";function m(e){let{components:t,...n}=e;return(0,o.kt)(p,(0,a.Z)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)(r.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"pip",label:"Pip",default:!0,mdxType:"TabItem"},(0,o.kt)(s.Z,{language:"bash",mdxType:"CodeBlock"},"pip install langchain")),(0,o.kt)(l.Z,{value:"conda",label:"Conda",mdxType:"TabItem"},(0,o.kt)(s.Z,{language:"bash",mdxType:"CodeBlock"},"conda install langchain -c conda-forge"))))}m.isMDXComponent=!0;const u={toc:[]},c="wrapper";function h(e){let{components:t,...n}=e;return(0,o.kt)(c,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"First we'll need to install their Python package:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install openai\n")),(0,o.kt)("p",null,"Accessing the API requires an API key, which you can get by creating an account and heading ",(0,o.kt)("a",{parentName:"p",href:"https://platform.openai.com/account/api-keys"},"here"),". Once we have a key we'll want to set it as an environment variable by running:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'export OPENAI_API_KEY="..."\n')),(0,o.kt)("p",null,"If you'd prefer not to set an environment variable you can pass the key in directly via the ",(0,o.kt)("inlineCode",{parentName:"p"},"openai_api_key")," named parameter when initiating the OpenAI LLM class:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.llms import OpenAI\n\nllm = OpenAI(openai_api_key="...")\n')))}h.isMDXComponent=!0;const d={toc:[]},g="wrapper";function y(e){let{components:t,...n}=e;return(0,o.kt)(g,(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0.9)\n")),(0,o.kt)("p",null,"And now we can pass in text and get predictions!"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'llm.predict("What would be a good company name for a company that makes colorful socks?")\n# >> Feetful of Fun\n')))}y.isMDXComponent=!0;const k={toc:[]},f="wrapper";function w(e){let{components:t,...n}=e;return(0,o.kt)(f,(0,a.Z)({},k,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nchat = ChatOpenAI(temperature=0)\nchat.predict_messages([HumanMessage(content="Translate this sentence from English to French. I love programming.")])\n# >> AIMessage(content="J\'aime programmer.", additional_kwargs={})\n')),(0,o.kt)("p",null,"It is useful to understand how chat models are different from a normal LLM, but it can often be handy to just be able to treat them the same.\nLangChain makes that easy by also exposing an interface through which you can interact with a chat model as you would a normal LLM.\nYou can access this through the ",(0,o.kt)("inlineCode",{parentName:"p"},"predict")," interface."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'chat.predict("Translate this sentence from English to French. I love programming.")\n# >> J\'aime programmer\n')))}w.isMDXComponent=!0;const b={toc:[]},v="wrapper";function T(e){let{components:t,...n}=e;return(0,o.kt)(v,(0,a.Z)({},b,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")\nprompt.format(product="colorful socks")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},"What is a good name for a company that makes colorful socks?\n")))}T.isMDXComponent=!0;const I={toc:[]},C="wrapper";function N(e){let{components:t,...n}=e;return(0,o.kt)(C,(0,a.Z)({},I,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\ntemplate = "You are a helpful assistant that translates {input_language} to {output_language}."\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = "{text}"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n\nchat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},'[\n    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),\n    HumanMessage(content="I love programming.")\n]\n')))}N.isMDXComponent=!0;const M={toc:[]},_="wrapper";function A(e){let{components:t,...n}=e;return(0,o.kt)(_,(0,a.Z)({},M,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Using this we can replace"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'llm.predict("What would be a good company name for a company that makes colorful socks?")\n')),(0,o.kt)("p",null,"with"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chains import LLMChain\n\nchain = LLMChain(llm=llm, prompt=prompt)\nchain.run("colorful socks")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},"Feetful of Fun\n")))}A.isMDXComponent=!0;const x={toc:[]},L="wrapper";function E(e){let{components:t,...n}=e;return(0,o.kt)(L,(0,a.Z)({},x,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\nchat = ChatOpenAI(temperature=0)\n\ntemplate = "You are a helpful assistant that translates {input_language} to {output_language}."\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = "{text}"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nchain.run(input_language="English", output_language="French", text="I love programming.")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},"J'aime programmer.\n")))}E.isMDXComponent=!0;const P={toc:[]},O="wrapper";function S(e){let{components:t,...n}=e;return(0,o.kt)(O,(0,a.Z)({},P,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n\n# The language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n\n# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n\n# Let's test it out!\nagent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},'> Entering new AgentExecutor chain...\n\nThought: I need to find the temperature first, then use the calculator to raise it to the .023 power.\nAction: Search\nAction Input: "High temperature in SF yesterday"\nObservation: San Francisco Temperature Yesterday. Maximum temperature yesterday: 57 \xb0F (at 1:56 pm) Minimum temperature yesterday: 49 \xb0F (at 1:56 am) Average temperature ...\n\nThought: I now have the temperature, so I can use the calculator to raise it to the .023 power.\nAction: Calculator\nAction Input: 57^.023\nObservation: Answer: 1.0974509573251117\n\nThought: I now know the final answer\nFinal Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.\n\n> Finished chain.\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},"The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.\n")))}S.isMDXComponent=!0;const Z={toc:[]},D="wrapper";function F(e){let{components:t,...n}=e;return(0,o.kt)(D,(0,a.Z)({},Z,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n\n# Now let's test it out!\nagent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},'> Entering new AgentExecutor chain...\nThought: I need to use a search engine to find Olivia Wilde\'s boyfriend and a calculator to raise his age to the 0.23 power.\nAction:\n{\n    "action": "Search",\n    "action_input": "Olivia Wilde boyfriend"\n}\n\nObservation: Sudeikis and Wilde\'s relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don\'t Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don\'t Worry Darling.\nThought:I need to use a search engine to find Harry Styles\' current age.\nAction:\n{\n    "action": "Search",\n    "action_input": "Harry Styles age"\n}\n\nObservation: 29 years\nThought:Now I need to calculate 29 raised to the 0.23 power.\nAction:\n{\n    "action": "Calculator",\n    "action_input": "29^0.23"\n}\n\nObservation: Answer: 2.169459462491557\n\nThought:I now know the final answer.\nFinal Answer: 2.169459462491557\n\n> Finished chain.\n\'2.169459462491557\'\n')))}F.isMDXComponent=!0;const H={toc:[]},q="wrapper";function X(e){let{components:t,...n}=e;return(0,o.kt)(q,(0,a.Z)({},H,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain import OpenAI, ConversationChain\n\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\n\nconversation.run("Hi there!")\n')),(0,o.kt)("p",null,"here's what's going on under the hood"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},"> Entering new chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hi there!\nAI:\n\n> Finished chain.\n\n>> 'Hello! How are you today?'\n")),(0,o.kt)("p",null,"Now if we run the chain again"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'conversation.run("I\'m doing well! Just having a conversation with an AI.")\n')),(0,o.kt)("p",null,"we'll see that the full prompt that's passed to the model contains the input and output of our first interaction, along with our latest input"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},"> Entering new chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hi there!\nAI:  Hello! How are you today?\nHuman: I'm doing well! Just having a conversation with an AI.\nAI:\n\n> Finished chain.\n\n>> \"That's great! What would you like to talk about?\"\n")))}X.isMDXComponent=!0;const V={toc:[]},W="wrapper";function R(e){let{components:t,...n}=e;return(0,o.kt)(W,(0,a.Z)({},V,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.prompts import (\n    ChatPromptTemplate,\n    MessagesPlaceholder,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate\n)\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\n\nprompt = ChatPromptTemplate.from_messages([\n    SystemMessagePromptTemplate.from_template(\n        "The following is a friendly conversation between a human and an AI. The AI is talkative and "\n        "provides lots of specific details from its context. If the AI does not know the answer to a "\n        "question, it truthfully says it does not know."\n    ),\n    MessagesPlaceholder(variable_name="history"),\n    HumanMessagePromptTemplate.from_template("{input}")\n])\n\nllm = ChatOpenAI(temperature=0)\nmemory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n\nconversation.predict(input="Hi there!")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},"Hello! How can I assist you today?\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'conversation.predict(input="I\'m doing well! Just having a conversation with an AI.")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'conversation.predict(input="Tell me about yourself.")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\n")))}R.isMDXComponent=!0;const B={},z="Quickstart",Y={unversionedId:"get_started/quickstart",id:"get_started/quickstart",title:"Quickstart",description:"Installation",source:"@site/docs/get_started/quickstart.mdx",sourceDirName:"get_started",slug:"/get_started/quickstart",permalink:"/langchain-docs-scratch/docs/get_started/quickstart",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/get_started/quickstart.mdx",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Installation",permalink:"/langchain-docs-scratch/docs/get_started/installation"},next:{title:"Model I/O",permalink:"/langchain-docs-scratch/docs/modules/model_io/"}},j={},J=[{value:"Installation",id:"installation",level:2},{value:"Environment setup",id:"environment-setup",level:2},{value:"Building an application",id:"building-an-application",level:2},{value:"LLMs",id:"llms",level:2},{value:"Get predictions from a language model",id:"get-predictions-from-a-language-model",level:4},{value:"Chat models",id:"chat-models",level:2},{value:"Prompt templates",id:"prompt-templates",level:2},{value:"Chains",id:"chains",level:2},{value:"Agents",id:"agents",level:2},{value:"Memory",id:"memory",level:2}],U={toc:J},$="wrapper";function G(e){let{components:t,...n}=e;return(0,o.kt)($,(0,a.Z)({},U,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"quickstart"},"Quickstart"),(0,o.kt)("h2",{id:"installation"},"Installation"),(0,o.kt)("p",null,"To install LangChain run:"),(0,o.kt)(m,{mdxType:"Install"}),(0,o.kt)("p",null,"For more details, see our ",(0,o.kt)("a",{parentName:"p",href:"/langchain-docs-scratch/docs/get_started/installation"},"Installation Guide"),"."),(0,o.kt)("h2",{id:"environment-setup"},"Environment setup"),(0,o.kt)("p",null,"Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs."),(0,o.kt)(h,{mdxType:"OpenAISetup"}),(0,o.kt)("h2",{id:"building-an-application"},"Building an application"),(0,o.kt)("p",null,"Now we can start building our language model application. LangChain provides many modules that can be used to build language model applications. Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases."),(0,o.kt)("h2",{id:"llms"},"LLMs"),(0,o.kt)("h4",{id:"get-predictions-from-a-language-model"},"Get predictions from a language model"),(0,o.kt)("p",null,"The basic building block of LangChain is the LLM, which takes in text and generates more text."),(0,o.kt)("p",null,"As an example, suppose we're building an application that generates a company name based on a company description. In order to do this, we need to initialize an OpenAI model wrapper. In this case, since we want the outputs to be MORE random, we'll initialize our model with a HIGH temperature."),(0,o.kt)(y,{mdxType:"LLM"}),(0,o.kt)("h2",{id:"chat-models"},"Chat models"),(0,o.kt)("p",null,'Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a "text in, text out" API, they expose an interface where "chat messages" are the inputs and outputs.'),(0,o.kt)("p",null,"You can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are ",(0,o.kt)("inlineCode",{parentName:"p"},"AIMessage"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"HumanMessage"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"SystemMessage"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatMessage")," -- ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatMessage")," takes in an arbitrary role parameter. Most of the time, you'll just be dealing with ",(0,o.kt)("inlineCode",{parentName:"p"},"HumanMessage"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"AIMessage"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"SystemMessage"),"."),(0,o.kt)(w,{mdxType:"ChatModel"}),(0,o.kt)("h2",{id:"prompt-templates"},"Prompt templates"),(0,o.kt)("p",null,"Most LLM applications do not pass user input directly into to an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand."),(0,o.kt)("p",null,"In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions."),(0,o.kt)(r.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"llms",label:"LLMs",default:!0,mdxType:"TabItem"},(0,o.kt)("p",null,"With PromptTemplates this is easy! In this case our template would be very simple:"),(0,o.kt)(T,{mdxType:"PromptTemplateLLM"})),(0,o.kt)(l.Z,{value:"chat_models",label:"Chat models",mdxType:"TabItem"},(0,o.kt)("p",null,"Similar to LLMs, you can make use of templating by using a ",(0,o.kt)("inlineCode",{parentName:"p"},"MessagePromptTemplate"),". You can build a ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatPromptTemplate")," from one or more ",(0,o.kt)("inlineCode",{parentName:"p"},"MessagePromptTemplate"),"s. You can use ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatPromptTemplate"),"'s ",(0,o.kt)("inlineCode",{parentName:"p"},"format_messages")," method to generate the formatted messages."),(0,o.kt)("p",null,"Because this is generating a list of messages, it is slightly more complex than the normal prompt template which is generating only a string. Please see the detailed guides on prompts to understand more options available to you here."),(0,o.kt)(N,{mdxType:"PromptTemplateChatModel"}))),(0,o.kt)("h2",{id:"chains"},"Chains"),(0,o.kt)("p",null,"Now that we've got a model and a prompt template, we'll want to combine the two. Chains give us a way to link (or chain) together multiple primitives, like models, prompts, and other chains."),(0,o.kt)(r.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"llms",label:"LLMs",default:!0,mdxType:"TabItem"},(0,o.kt)("p",null,"The simplest and most common type of chain is an LLMChain, which passes an input first to a PromptTemplate and then to an LLM. We can construct an LLM chain from our existing model and prompt template."),(0,o.kt)(A,{mdxType:"ChainLLM"}),(0,o.kt)("p",null,"There we go, our first chain! Understanding how this simple chain works will set you up well for working with more complex chains.")),(0,o.kt)(l.Z,{value:"chat_models",label:"Chat models",mdxType:"TabItem"},(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"LLMChain")," can be used with chat models as well:"),(0,o.kt)(E,{mdxType:"ChainChatModel"}))),(0,o.kt)("h2",{id:"agents"},"Agents"),(0,o.kt)("p",null,"Our first chain ran a pre-determined sequence of steps. To handle complex workflows, we need to be able to dynamically choose actions based on inputs."),(0,o.kt)("p",null,"Agents do just this: they use a language model to determine which actions to take and in what order. Agents are given access to tools, and they repeatedly choose a tool, run the tool, and observe the output until they come up with a final answer."),(0,o.kt)("p",null,"To load an agent, you need to choose a(n):"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"LLM/Chat model: The language model powering the agent."),(0,o.kt)("li",{parentName:"ul"},"Tool(s): A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. For a list of predefined tools and their specifications, see ",(0,o.kt)("a",{parentName:"li",href:"../modules/agents/tools/getting_started.md"},"here"),"."),(0,o.kt)("li",{parentName:"ul"},"Agent name: A string that references a supported agent class. An agent class is largely parameterized by the prompt the language model uses to determine which action to take. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see the documentation for custom agents (coming soon). For a list of supported agents and their specifications, see ",(0,o.kt)("a",{parentName:"li",href:"../modules/agents.html"},"here"),".")),(0,o.kt)("p",null,"For this example, we'll be using SerpAPI to query a search engine."),(0,o.kt)("p",null,"You'll need to install the SerpAPI Python package:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install google-search-results\n")),(0,o.kt)("p",null,"And set the ",(0,o.kt)("inlineCode",{parentName:"p"},"SERPAPI_API_KEY")," environment variable."),(0,o.kt)(r.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"llms",label:"LLMs",default:!0,mdxType:"TabItem"},(0,o.kt)(S,{mdxType:"AgentLLM"})),(0,o.kt)(l.Z,{value:"chat_models",label:"Chat models",mdxType:"TabItem"},(0,o.kt)("p",null,"Agents can also be used with chat models, you can initialize one using ",(0,o.kt)("inlineCode",{parentName:"p"},"AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION")," as the agent type."),(0,o.kt)(F,{mdxType:"AgentChatModel"}))),(0,o.kt)("h2",{id:"memory"},"Memory"),(0,o.kt)("p",null,"The chains and agents we've looked at so far have been stateless, but for many applications it's necessary to reference past interactions. This is clearly the case with a chatbot for example, where you want it to understand new messages in the context of past messages."),(0,o.kt)("p",null,"The Memory module gives you a way to maintain application state. The base Memory interface is simple: it lets you update state given the latest run inputs and outputs and it lets you modify (or contextualize) the next input using the stored state."),(0,o.kt)("p",null,"There are a number of built-in memory systems. The simplest of these are is a buffer memory which just prepends the last few inputs/outputs to the current input - we will use this in the example below."),(0,o.kt)(r.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"llms",label:"LLMs",default:!0,mdxType:"TabItem"},(0,o.kt)(X,{mdxType:"MemoryLLM"})),(0,o.kt)(l.Z,{value:"chat_models",label:"Chat models",mdxType:"TabItem"},(0,o.kt)("p",null,"You can use Memory with chains and agents initialized with chat models. The main difference between this and Memory for LLMs is that rather than trying to condense all previous messages into a string, we can keep them as their own unique memory object."),(0,o.kt)(R,{mdxType:"MemoryChatModel"}))))}G.isMDXComponent=!0}}]);