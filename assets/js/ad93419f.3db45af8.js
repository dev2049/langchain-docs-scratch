"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[84762],{3905:(t,e,n)=>{n.d(e,{Zo:()=>c,kt:()=>m});var r=n(67294);function o(t,e,n){return e in t?Object.defineProperty(t,e,{value:n,enumerable:!0,configurable:!0,writable:!0}):t[e]=n,t}function a(t,e){var n=Object.keys(t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(t);e&&(r=r.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),n.push.apply(n,r)}return n}function i(t){for(var e=1;e<arguments.length;e++){var n=null!=arguments[e]?arguments[e]:{};e%2?a(Object(n),!0).forEach((function(e){o(t,e,n[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(n,e))}))}return t}function s(t,e){if(null==t)return{};var n,r,o=function(t,e){if(null==t)return{};var n,r,o={},a=Object.keys(t);for(r=0;r<a.length;r++)n=a[r],e.indexOf(n)>=0||(o[n]=t[n]);return o}(t,e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(t);for(r=0;r<a.length;r++)n=a[r],e.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(t,n)&&(o[n]=t[n])}return o}var l=r.createContext({}),p=function(t){var e=r.useContext(l),n=e;return t&&(n="function"==typeof t?t(e):i(i({},e),t)),n},c=function(t){var e=p(t.components);return r.createElement(l.Provider,{value:e},t.children)},u="mdxType",d={inlineCode:"code",wrapper:function(t){var e=t.children;return r.createElement(r.Fragment,{},e)}},k=r.forwardRef((function(t,e){var n=t.components,o=t.mdxType,a=t.originalType,l=t.parentName,c=s(t,["components","mdxType","originalType","parentName"]),u=p(n),k=o,m=u["".concat(l,".").concat(k)]||u[k]||d[k]||a;return n?r.createElement(m,i(i({ref:e},c),{},{components:n})):r.createElement(m,i({ref:e},c))}));function m(t,e){var n=arguments,o=e&&e.mdxType;if("string"==typeof t||o){var a=n.length,i=new Array(a);i[0]=k;var s={};for(var l in e)hasOwnProperty.call(e,l)&&(s[l]=e[l]);s.originalType=t,s[u]="string"==typeof t?t:o,i[1]=s;for(var p=2;p<a;p++)i[p]=n[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}k.displayName="MDXCreateElement"},3639:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>a,metadata:()=>s,toc:()=>p});var r=n(87462),o=(n(67294),n(3905));const a={},i="Split by tokens (tiktoken)",s={unversionedId:"modules/data_io/text_splitters/how_to/tiktoken",id:"modules/data_io/text_splitters/how_to/tiktoken",title:"Split by tokens (tiktoken)",description:"tiktoken is a fast BPE tokenizer created by OpenAI.",source:"@site/docs/modules/data_io/text_splitters/how_to/tiktoken.md",sourceDirName:"modules/data_io/text_splitters/how_to",slug:"/modules/data_io/text_splitters/how_to/tiktoken",permalink:"/langchain-docs-scratch/docs/modules/data_io/text_splitters/how_to/tiktoken",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/data_io/text_splitters/how_to/tiktoken.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Split by tokens (spaCy)",permalink:"/langchain-docs-scratch/docs/modules/data_io/text_splitters/how_to/spacy"},next:{title:"Vector stores",permalink:"/langchain-docs-scratch/docs/modules/data_io/vectorstores/"}},l={},p=[],c=(u="CodeOutputBlock",function(t){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",t)});var u;const d={toc:p},k="wrapper";function m(t){let{components:e,...n}=t;return(0,o.kt)(k,(0,r.Z)({},d,n,{components:e,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"split-by-tokens-tiktoken"},"Split by tokens (tiktoken)"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("a",{parentName:"p",href:"https://github.com/openai/tiktoken"},"tiktoken")," is a fast ",(0,o.kt)("inlineCode",{parentName:"p"},"BPE")," tokenizer created by ",(0,o.kt)("inlineCode",{parentName:"p"},"OpenAI"),".")),(0,o.kt)("p",null,"We can use it to estimate tokens used. It will probably be more accurate for the OpenAI models."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"How the text is split: by character passed in"),(0,o.kt)("li",{parentName:"ol"},"How the chunk size is measured: by ",(0,o.kt)("inlineCode",{parentName:"li"},"tiktoken")," tokenizer")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"#!pip install tiktoken\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# This is a long document we can split up.\nwith open('../../../state_of_the_union.txt') as f:\n    state_of_the_union = f.read()\nfrom langchain.text_splitter import CharacterTextSplitter\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)\ntexts = text_splitter.split_text(state_of_the_union)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"print(texts[0])\n")),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n    \n    Last year COVID-19 kept us apart. This year we are finally together again. \n    \n    Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n    \n    With a duty to one another to the American people to the Constitution.\n"))),(0,o.kt)("p",null,"We can also load a tiktoken splitter directly"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n\ntexts = text_splitter.split_text(state_of_the_union)\nprint(texts[0])\n")))}m.isMDXComponent=!0}}]);