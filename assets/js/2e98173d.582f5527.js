"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[27998],{3905:(e,t,a)=>{a.d(t,{Zo:()=>i,kt:()=>d});var o=a(67294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function m(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,o,n=function(e,t){if(null==e)return{};var a,o,n={},r=Object.keys(e);for(o=0;o<r.length;o++)a=r[o],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)a=r[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var p=o.createContext({}),l=function(e){var t=o.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):m(m({},t),e)),a},i=function(e){var t=l(e.components);return o.createElement(p.Provider,{value:t},e.children)},c="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var a=e.components,n=e.mdxType,r=e.originalType,p=e.parentName,i=s(e,["components","mdxType","originalType","parentName"]),c=l(a),u=n,d=c["".concat(p,".").concat(u)]||c[u]||h[u]||r;return a?o.createElement(d,m(m({ref:t},i),{},{components:a})):o.createElement(d,m({ref:t},i))}));function d(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=a.length,m=new Array(r);m[0]=u;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[c]="string"==typeof e?e:n,m[1]=s;for(var l=2;l<r;l++)m[l]=a[l];return o.createElement.apply(null,m)}return o.createElement.apply(null,a)}u.displayName="MDXCreateElement"},64395:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>m,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var o=a(87462),n=(a(67294),a(3905));const r={},m="Few shot examples for chat models",s={unversionedId:"modules/model_io/prompts/prompt_templates/few_shot_examples_chat",id:"modules/model_io/prompts/prompt_templates/few_shot_examples_chat",title:"Few shot examples for chat models",description:"This notebook covers how to use few shot examples in chat models.",source:"@site/docs/modules/model_io/prompts/prompt_templates/few_shot_examples_chat.md",sourceDirName:"modules/model_io/prompts/prompt_templates",slug:"/modules/model_io/prompts/prompt_templates/few_shot_examples_chat",permalink:"/langchain-docs-scratch/docs/modules/model_io/prompts/prompt_templates/few_shot_examples_chat",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/model_io/prompts/prompt_templates/few_shot_examples_chat.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Few-shot prompt templates",permalink:"/langchain-docs-scratch/docs/modules/model_io/prompts/prompt_templates/few_shot_examples"},next:{title:"Partial prompt templates",permalink:"/langchain-docs-scratch/docs/modules/model_io/prompts/prompt_templates/partial"}},p={},l=[{value:"Alternating Human/AI messages",id:"alternating-humanai-messages",level:2},{value:"System Messages",id:"system-messages",level:2}],i=(c="CodeOutputBlock",function(e){return console.warn("Component "+c+" was not imported, exported, or provided by MDXProvider as global scope"),(0,n.kt)("div",e)});var c;const h={toc:l},u="wrapper";function d(e){let{components:t,...a}=e;return(0,n.kt)(u,(0,o.Z)({},h,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"few-shot-examples-for-chat-models"},"Few shot examples for chat models"),(0,n.kt)("p",null,"This notebook covers how to use few shot examples in chat models."),(0,n.kt)("p",null,"There does not appear to be solid consensus on how best to do few shot prompting. As a result, we are not solidifying any abstractions around this yet but rather using existing abstractions."),(0,n.kt)("h2",{id:"alternating-humanai-messages"},"Alternating Human/AI messages"),(0,n.kt)("p",null,"The first way of doing few shot prompting relies on using alternating human/ai messages. See an example of this below."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.chat_models import ChatOpenAI\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"chat = ChatOpenAI(temperature=0)\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'template="You are a helpful assistant that translates english to pirate."\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nexample_human = HumanMessagePromptTemplate.from_template("Hi")\nexample_ai = AIMessagePromptTemplate.from_template("Argh me mateys")\nhuman_template="{text}"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\nchain = LLMChain(llm=chat, prompt=chat_prompt)\n# get a chat completion from the formatted messages\nchain.run("I love programming.")\n')),(0,n.kt)(i,{lang:"python",mdxType:"CodeOutputBlock"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"    \"I be lovin' programmin', me hearty!\"\n"))),(0,n.kt)("h2",{id:"system-messages"},"System Messages"),(0,n.kt)("p",null,"OpenAI provides an optional ",(0,n.kt)("inlineCode",{parentName:"p"},"name")," parameter that they also recommend using in conjunction with system messages to do few shot prompting. Here is an example of how to do that below."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'template="You are a helpful assistant that translates english to pirate."\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nexample_human = SystemMessagePromptTemplate.from_template("Hi", additional_kwargs={"name": "example_user"})\nexample_ai = SystemMessagePromptTemplate.from_template("Argh me mateys", additional_kwargs={"name": "example_assistant"})\nhuman_template="{text}"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\nchain = LLMChain(llm=chat, prompt=chat_prompt)\n# get a chat completion from the formatted messages\nchain.run("I love programming.")\n')),(0,n.kt)(i,{lang:"python",mdxType:"CodeOutputBlock"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"    \"I be lovin' programmin', me hearty.\"\n"))))}d.isMDXComponent=!0}}]);