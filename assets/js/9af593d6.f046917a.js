"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[65304],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>h});var o=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,o,r=function(e,n){if(null==e)return{};var t,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var p=o.createContext({}),i=function(e){var n=o.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},u=function(e){var n=i(e.components);return o.createElement(p.Provider,{value:n},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},c=o.forwardRef((function(e,n){var t=e.components,r=e.mdxType,a=e.originalType,p=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=i(t),c=r,h=m["".concat(p,".").concat(c)]||m[c]||d[c]||a;return t?o.createElement(h,l(l({ref:n},u),{},{components:t})):o.createElement(h,l({ref:n},u))}));function h(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var a=t.length,l=new Array(a);l[0]=c;var s={};for(var p in n)hasOwnProperty.call(n,p)&&(s[p]=n[p]);s.originalType=e,s[m]="string"==typeof e?e:r,l[1]=s;for(var i=2;i<a;i++)l[i]=t[i];return o.createElement.apply(null,l)}return o.createElement.apply(null,t)}c.displayName="MDXCreateElement"},3430:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>i});var o=t(87462),r=(t(67294),t(3905));const a={},l="Runhouse",s={unversionedId:"modules/model_io/models/llms/integrations/runhouse",id:"modules/model_io/models/llms/integrations/runhouse",title:"Runhouse",description:"The Runhouse allows remote compute and data across environments and users. See the Runhouse docs.",source:"@site/docs/modules/model_io/models/llms/integrations/runhouse.md",sourceDirName:"modules/model_io/models/llms/integrations",slug:"/modules/model_io/models/llms/integrations/runhouse",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/runhouse",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/model_io/models/llms/integrations/runhouse.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Replicate",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/replicate"},next:{title:"SageMakerEndpoint",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/sagemaker"}},p={},i=[],u=(m="CodeOutputBlock",function(e){return console.warn("Component "+m+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var m;const d={toc:i},c="wrapper";function h(e){let{components:n,...t}=e;return(0,r.kt)(c,(0,o.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"runhouse"},"Runhouse"),(0,r.kt)("p",null,"The ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/run-house/runhouse"},"Runhouse")," allows remote compute and data across environments and users. See the ",(0,r.kt)("a",{parentName:"p",href:"https://runhouse-docs.readthedocs-hosted.com/en/latest/"},"Runhouse docs"),"."),(0,r.kt)("p",null,"This example goes over how to use LangChain and ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/run-house/runhouse"},"Runhouse")," to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Note"),": Code uses ",(0,r.kt)("inlineCode",{parentName:"p"},"SelfHosted")," name instead of the ",(0,r.kt)("inlineCode",{parentName:"p"},"Runhouse"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"pip install runhouse\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\nfrom langchain import PromptTemplate, LLMChain\nimport runhouse as rh\n")),(0,r.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs\n"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# For an on-demand A100 with GCP, Azure, or Lambda\ngpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\", use_spot=False)\n\n# For an on-demand A10G with AWS (no single A100s on AWS)\n# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')\n\n# For an existing cluster\n# gpu = rh.cluster(ips=['<ip of the cluster>'], \n#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},\n#                  name='rh-a10x')\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'template = """Question: {question}\n\nAnswer: Let\'s think step by step."""\n\nprompt = PromptTemplate(template=template, input_variables=["question"])\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'llm = SelfHostedHuggingFaceLLM(model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"])\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"llm_chain = LLMChain(prompt=prompt, llm=llm)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"\n\nllm_chain.run(question)\n')),(0,r.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC\n    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds\n\n\n\n\n\n    \"\\n\\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber\"\n"))),(0,r.kt)("p",null,"You can also load more custom models through the SelfHostedHuggingFaceLLM interface:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'llm = SelfHostedHuggingFaceLLM(\n    model_id="google/flan-t5-small",\n    task="text2text-generation",\n    hardware=gpu,\n)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'llm("What is the capital of Germany?")\n')),(0,r.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC\n    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds\n\n\n\n\n\n    'berlin'\n"))),(0,r.kt)("p",null,"Using a custom load function, we can load a custom pipeline directly on the remote hardware:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def load_pipeline():\n    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # Need to be inside the fn in notebooks\n    model_id = "gpt2"\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(model_id)\n    pipe = pipeline(\n        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10\n    )\n    return pipe\n\ndef inference_fn(pipeline, prompt, stop = None):\n    return pipeline(prompt)[0]["generated_text"][len(prompt):]\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"llm = SelfHostedHuggingFaceLLM(model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'llm("Who is the current US president?")\n')),(0,r.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC\n    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds\n\n\n\n\n\n    'john w. bush'\n"))),(0,r.kt)("p",null,"You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"pipeline = load_pipeline()\nllm = SelfHostedPipeline.from_pipeline(\n    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs\n)\n")),(0,r.kt)("p",null,"Instead, we can also send it to the hardware's filesystem, which will be much faster."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(gpu, path="models")\n\nllm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu)\n')))}h.isMDXComponent=!0}}]);