"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[58819],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>d});var a=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var p=a.createContext({}),s=function(e){var n=a.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},c=function(e){var n=s(e.components);return a.createElement(p.Provider,{value:n},e.children)},h="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,o=e.mdxType,i=e.originalType,p=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),h=s(t),m=o,d=h["".concat(p,".").concat(m)]||h[m]||u[m]||i;return t?a.createElement(d,r(r({ref:n},c),{},{components:t})):a.createElement(d,r({ref:n},c))}));function d(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var i=t.length,r=new Array(i);r[0]=m;var l={};for(var p in n)hasOwnProperty.call(n,p)&&(l[p]=n[p]);l.originalType=e,l[h]="string"==typeof e?e:o,r[1]=l;for(var s=2;s<i;s++)r[s]=t[s];return a.createElement.apply(null,r)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},22414:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var a=t(87462),o=(t(67294),t(3905));const i={sidebar_position:2},r="Chains",l={unversionedId:"modules/chains/index",id:"modules/chains/index",title:"Chains",description:"Using an LLM in isolation is fine for some simple applications,",source:"@site/docs/modules/chains/index.mdx",sourceDirName:"modules/chains",slug:"/modules/chains/",permalink:"/langchain-docs-scratch/docs/modules/chains/",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/chains/index.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"sidebar",previous:{title:"Zep",permalink:"/langchain-docs-scratch/docs/modules/data_io/retrievers/integrations/zep_memorystore"},next:{title:"Async API",permalink:"/langchain-docs-scratch/docs/modules/chains/how_to/async_chain"}},p={},s=[{value:"Why do we need chains?",id:"why-do-we-need-chains",level:2},{value:"Quickstart: Using <code>LLMChain</code>",id:"quickstart-using-llmchain",level:2},{value:"Different ways of calling chains",id:"different-ways-of-calling-chains",level:2},{value:"Adding memory to chains",id:"adding-memory-to-chains",level:2},{value:"Debugging chains",id:"debugging-chains",level:2},{value:"Combine chains with the <code>SequentialChain</code>",id:"combine-chains-with-the-sequentialchain",level:2},{value:"Create a custom chain with the <code>Chain</code> class",id:"create-a-custom-chain-with-the-chain-class",level:2}],c=(h="CodeOutputBlock",function(e){return console.warn("Component "+h+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var h;const u={toc:s},m="wrapper";function d(e){let{components:n,...i}=e;return(0,o.kt)(m,(0,a.Z)({},u,i,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"chains"},"Chains"),(0,o.kt)("p",null,"Using an LLM in isolation is fine for some simple applications,\nbut more complex applications require chaining LLMs - either with each other or with other experts.\nLangChain provides a standard interface for ",(0,o.kt)("strong",{parentName:"p"},"Chains"),", as well as several common implementations of chains."),(0,o.kt)("p",null,"For more on specifics check out:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"./how_to/async_chain.html"},"Basics")," to get acquainted with foundational chains"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"./index_examples/analyze_document.html"},"Chains + data")," to see how to make chains that interact with external data"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"./integrations/api.html"},"Advanced")," to see some of the more advanced chains and integrations that you can use out of the box")),(0,o.kt)("h2",{id:"why-do-we-need-chains"},"Why do we need chains?"),(0,o.kt)("p",null,"Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components."),(0,o.kt)("h2",{id:"quickstart-using-llmchain"},"Quickstart: Using ",(0,o.kt)("inlineCode",{parentName:"h2"},"LLMChain")),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"LLMChain")," is a simple chain that takes in a prompt template, formats it with the user input and returns the response from an LLM."),(0,o.kt)("p",null,"To use the ",(0,o.kt)("inlineCode",{parentName:"p"},"LLMChain"),", first create a prompt template."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=["product"],\n    template="What is a good name for a company that makes {product}?",\n)\n')),(0,o.kt)("p",null,"We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\n\n# Run the chain only specifying the input variable.\nprint(chain.run("colorful socks"))\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    \n    \n    Colorful Toes Co.\n"))),(0,o.kt)("p",null,"If there are multiple variables, you can input them all at once using a dictionary."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'prompt = PromptTemplate(\n    input_variables=["company", "product"],\n    template="What is a good name for {company} that makes {product}?",\n)\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run({\n    \'company\': "ABC Startup",\n    \'product\': "colorful socks"\n    }))\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    \n    \n    Socktopia Colourful Creations.\n"))),(0,o.kt)("p",null,"You can use a chat model in an ",(0,o.kt)("inlineCode",{parentName:"p"},"LLMChain")," as well:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n)\nhuman_message_prompt = HumanMessagePromptTemplate(\n        prompt=PromptTemplate(\n            template="What is a good name for a company that makes {product}?",\n            input_variables=["product"],\n        )\n    )\nchat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\nchat = ChatOpenAI(temperature=0.9)\nchain = LLMChain(llm=chat, prompt=chat_prompt_template)\nprint(chain.run("colorful socks"))\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Rainbow Socks Co.\n"))),(0,o.kt)("h2",{id:"different-ways-of-calling-chains"},"Different ways of calling chains"),(0,o.kt)("p",null,"All classes inherited from ",(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," offer a few ways of running chain logic. The most direct one is by using ",(0,o.kt)("inlineCode",{parentName:"p"},"__call__"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'chat = ChatOpenAI(temperature=0)\nprompt_template = "Tell me a {adjective} joke"\nllm_chain = LLMChain(\n    llm=chat,\n    prompt=PromptTemplate.from_template(prompt_template)\n)\n\nllm_chain(inputs={"adjective":"corny"})\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    {'adjective': 'corny',\n     'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}\n"))),(0,o.kt)("p",null,"By default, ",(0,o.kt)("inlineCode",{parentName:"p"},"__call__")," returns both the input and output key values. You can configure it to only return output key values by setting ",(0,o.kt)("inlineCode",{parentName:"p"},"return_only_outputs")," to ",(0,o.kt)("inlineCode",{parentName:"p"},"True"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'llm_chain("corny", return_only_outputs=True)\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    {'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}\n"))),(0,o.kt)("p",null,"If the ",(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," only outputs one output key (i.e. only has one element in its ",(0,o.kt)("inlineCode",{parentName:"p"},"output_keys"),"), you can  use ",(0,o.kt)("inlineCode",{parentName:"p"},"run")," method. Note that ",(0,o.kt)("inlineCode",{parentName:"p"},"run")," outputs a string instead of a dictionary."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# llm_chain only has one output key, so we can use run\nllm_chain.output_keys\n")),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    ['text']\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'llm_chain.run({"adjective":"corny"})\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    'Why did the tomato turn red? Because it saw the salad dressing!'\n"))),(0,o.kt)("p",null,"In the case of one input key, you can input the string directly without specifying the input mapping."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# These two are equivalent\nllm_chain.run({"adjective":"corny"})\nllm_chain.run("corny")\n\n# These two are also equivalent\nllm_chain("corny")\nllm_chain({"adjective":"corny"})\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    {'adjective': 'corny',\n     'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}\n"))),(0,o.kt)("p",null,"Tips: You can easily integrate a ",(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," object as a ",(0,o.kt)("inlineCode",{parentName:"p"},"Tool")," in your ",(0,o.kt)("inlineCode",{parentName:"p"},"Agent")," via its ",(0,o.kt)("inlineCode",{parentName:"p"},"run")," method. See an example ",(0,o.kt)("a",{target:"_blank",href:t(14568).Z},"here"),"."),(0,o.kt)("h2",{id:"adding-memory-to-chains"},"Adding memory to chains"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," supports taking a ",(0,o.kt)("inlineCode",{parentName:"p"},"BaseMemory")," object as its ",(0,o.kt)("inlineCode",{parentName:"p"},"memory")," argument, allowing ",(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," object to persist data across multiple calls. In other words, it makes ",(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," a stateful object."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nconversation = ConversationChain(\n    llm=chat,\n    memory=ConversationBufferMemory()\n)\n\nconversation.run("Answer briefly. What are the first 3 colors of a rainbow?")\n# -> The first three colors of a rainbow are red, orange, and yellow.\nconversation.run("And the next 4?")\n# -> The next four colors of a rainbow are green, blue, indigo, and violet.\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    'The next four colors of a rainbow are green, blue, indigo, and violet.'\n"))),(0,o.kt)("p",null,"Essentially, ",(0,o.kt)("inlineCode",{parentName:"p"},"BaseMemory")," defines an interface of how ",(0,o.kt)("inlineCode",{parentName:"p"},"langchain")," stores memory. It allows reading of stored data through ",(0,o.kt)("inlineCode",{parentName:"p"},"load_memory_variables")," method and storing new data through ",(0,o.kt)("inlineCode",{parentName:"p"},"save_context")," method. You can learn more about it in ",(0,o.kt)("a",{parentName:"p",href:"../memory.html"},"Memory")," section."),(0,o.kt)("h2",{id:"debugging-chains"},"Debugging chains"),(0,o.kt)("p",null,"It can be hard to debug a ",(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," object solely from its output as most ",(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," objects involve a fair amount of input prompt preprocessing and LLM output post-processing. Setting ",(0,o.kt)("inlineCode",{parentName:"p"},"verbose")," to ",(0,o.kt)("inlineCode",{parentName:"p"},"True")," will print out some internal states of the ",(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," object while it is being ran."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'conversation = ConversationChain(\n    llm=chat,\n    memory=ConversationBufferMemory(),\n    verbose=True\n)\nconversation.run("What is ChatGPT?")\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    \n    \n    > Entering new ConversationChain chain...\n    Prompt after formatting:\n    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n    \n    Current conversation:\n    \n    Human: What is ChatGPT?\n    AI:\n    \n    > Finished chain.\n\n\n\n\n\n    'ChatGPT is an AI language model developed by OpenAI. It is based on the GPT-3 architecture and is capable of generating human-like responses to text prompts. ChatGPT has been trained on a massive amount of text data and can understand and respond to a wide range of topics. It is often used for chatbots, virtual assistants, and other conversational AI applications.'\n"))),(0,o.kt)("h2",{id:"combine-chains-with-the-sequentialchain"},"Combine chains with the ",(0,o.kt)("inlineCode",{parentName:"h2"},"SequentialChain")),(0,o.kt)("p",null,"The next step after calling a language model is to make a series of calls to a language model. We can do this using sequential chains, which are chains that execute their links in a predefined order. Specifically, we will use the ",(0,o.kt)("inlineCode",{parentName:"p"},"SimpleSequentialChain"),". This is the simplest type of a sequential chain, where each step has a single input/output, and the output of one step is the input to the next."),(0,o.kt)("p",null,"In this tutorial, our sequential chain will:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"First, create a company name for a product. We will reuse the ",(0,o.kt)("inlineCode",{parentName:"li"},"LLMChain")," we'd previously initialized to create this company name."),(0,o.kt)("li",{parentName:"ol"},"Then, create a catchphrase for the product. We will initialize a new ",(0,o.kt)("inlineCode",{parentName:"li"},"LLMChain")," to create this catchphrase, as shown below.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'second_prompt = PromptTemplate(\n    input_variables=["company_name"],\n    template="Write a catchphrase for the following company: {company_name}",\n)\nchain_two = LLMChain(llm=llm, prompt=second_prompt)\n')),(0,o.kt)("p",null,"Now we can combine the two LLMChains, so that we can create a company name and a catchphrase in a single step."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chains import SimpleSequentialChain\noverall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n\n# Run the chain specifying only the input variable for the first chain.\ncatchphrase = overall_chain.run("colorful socks")\nprint(catchphrase)\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'    \n    \n    > Entering new SimpleSequentialChain chain...\n    Rainbow Socks Co.\n    \n    \n    "Put a little rainbow in your step!"\n    \n    > Finished chain.\n    \n    \n    "Put a little rainbow in your step!"\n'))),(0,o.kt)("h2",{id:"create-a-custom-chain-with-the-chain-class"},"Create a custom chain with the ",(0,o.kt)("inlineCode",{parentName:"h2"},"Chain")," class"),(0,o.kt)("p",null,"LangChain provides many chains out of the box, but sometimes you may want to create a custom chain for your specific use case. For this example, we will create a custom chain that concatenates the outputs of 2 ",(0,o.kt)("inlineCode",{parentName:"p"},"LLMChain"),"s."),(0,o.kt)("p",null,"In order to create a custom chain:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Start by subclassing the ",(0,o.kt)("inlineCode",{parentName:"li"},"Chain")," class,"),(0,o.kt)("li",{parentName:"ol"},"Fill out the ",(0,o.kt)("inlineCode",{parentName:"li"},"input_keys")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"output_keys")," properties,"),(0,o.kt)("li",{parentName:"ol"},"Add the ",(0,o.kt)("inlineCode",{parentName:"li"},"_call")," method that shows how to execute the chain.")),(0,o.kt)("p",null,"These steps are demonstrated in the example below:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.chains import LLMChain\nfrom langchain.chains.base import Chain\n\nfrom typing import Dict, List\n\n\nclass ConcatenateChain(Chain):\n    chain_1: LLMChain\n    chain_2: LLMChain\n\n    @property\n    def input_keys(self) -> List[str]:\n        # Union of the input keys of the two chains.\n        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n        return list(all_input_vars)\n\n    @property\n    def output_keys(self) -> List[str]:\n        return ['concat_output']\n\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n        output_1 = self.chain_1.run(inputs)\n        output_2 = self.chain_2.run(inputs)\n        return {'concat_output': output_1 + output_2}\n")),(0,o.kt)("p",null,"Now, we can try running the chain that we called."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'prompt_1 = PromptTemplate(\n    input_variables=["product"],\n    template="What is a good name for a company that makes {product}?",\n)\nchain_1 = LLMChain(llm=llm, prompt=prompt_1)\n\nprompt_2 = PromptTemplate(\n    input_variables=["product"],\n    template="What is a good slogan for a company that makes {product}?",\n)\nchain_2 = LLMChain(llm=llm, prompt=prompt_2)\n\nconcat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\nconcat_output = concat_chain.run("colorful socks")\nprint(f"Concatenated output:\\n{concat_output}")\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'    Concatenated output:\n    \n    \n    Funky Footwear Company\n    \n    "Brighten Up Your Day with Our Colorful Socks!"\n'))),(0,o.kt)("p",null,"That's it! For more details about how to do cool things with Chains, check out the ",(0,o.kt)("a",{parentName:"p",href:"how_to_guides.rst"},"how-to guide")," for chains."))}d.isMDXComponent=!0},14568:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/files/custom_tools-ec3fe179fa566ef902c39cf0c9a5452d.ipynb"}}]);