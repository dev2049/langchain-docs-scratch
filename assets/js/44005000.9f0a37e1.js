"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[25301],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>g});var a=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var i=a.createContext({}),c=function(e){var n=a.useContext(i),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(i.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(t),d=r,g=u["".concat(i,".").concat(d)]||u[d]||m[d]||o;return t?a.createElement(g,l(l({ref:n},p),{},{components:t})):a.createElement(g,l({ref:n},p))}));function g(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,l=new Array(o);l[0]=d;var s={};for(var i in n)hasOwnProperty.call(n,i)&&(s[i]=n[i]);s.originalType=e,s[u]="string"==typeof e?e:r,l[1]=s;for(var c=2;c<o;c++)l[c]=t[c];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},69894:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>l,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var a=t(87462),r=(t(67294),t(3905));const o={},l="Streaming final agent output",s={unversionedId:"modules/agents/how_to/streaming_stdout_final_only",id:"modules/agents/how_to/streaming_stdout_final_only",title:"Streaming final agent output",description:"If you only want the final output of an agent to be streamed, you can use the callback `FinalStreamingStdOutCallbackHandler`.",source:"@site/docs/modules/agents/how_to/streaming_stdout_final_only.md",sourceDirName:"modules/agents/how_to",slug:"/modules/agents/how_to/streaming_stdout_final_only",permalink:"/langchain-docs-scratch/docs/modules/agents/how_to/streaming_stdout_final_only",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/agents/how_to/streaming_stdout_final_only.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Shared memory across agents and tools",permalink:"/langchain-docs-scratch/docs/modules/agents/how_to/sharedmemory_for_tools"},next:{title:"Tools",permalink:"/langchain-docs-scratch/docs/modules/agents/tools/"}},i={},c=[{value:"Handling custom answer prefixes",id:"handling-custom-answer-prefixes",level:3},{value:"Also streaming the answer prefixes",id:"also-streaming-the-answer-prefixes",level:3}],p=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var u;const m={toc:c},d="wrapper";function g(e){let{components:n,...t}=e;return(0,r.kt)(d,(0,a.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"streaming-final-agent-output"},"Streaming final agent output"),(0,r.kt)("p",null,"If you only want the final output of an agent to be streamed, you can use the callback ",(0,r.kt)("inlineCode",{parentName:"p"},"FinalStreamingStdOutCallbackHandler"),".\nFor this, the underlying LLM has to support streaming as well."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler\nfrom langchain.llms import OpenAI\n")),(0,r.kt)("p",null,"Let's create the underlying LLM with ",(0,r.kt)("inlineCode",{parentName:"p"},"streaming = True")," and pass a new instance of ",(0,r.kt)("inlineCode",{parentName:"p"},"FinalStreamingStdOutCallbackHandler"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"llm = OpenAI(streaming=True, callbacks=[FinalStreamingStdOutCallbackHandler()], temperature=0)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'tools = load_tools(["wikipedia", "llm-math"], llm=llm)\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)\nagent.run("It\'s 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany.")\n')),(0,r.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"     Konrad Adenauer became Chancellor of Germany in 1949, 74 years ago in 2023.\n\n\n\n\n    'Konrad Adenauer became Chancellor of Germany in 1949, 74 years ago in 2023.'\n"))),(0,r.kt)("h3",{id:"handling-custom-answer-prefixes"},"Handling custom answer prefixes"),(0,r.kt)("p",null,"By default, we assume that the token sequence ",(0,r.kt)("inlineCode",{parentName:"p"},'"Final", "Answer", ":"')," indicates that the agent has reached an answers. We can, however, also pass a custom sequence to use as answer prefix."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'llm = OpenAI(\n    streaming=True,\n    callbacks=[FinalStreamingStdOutCallbackHandler(answer_prefix_tokens=["The", "answer", ":"])],\n    temperature=0\n)\n')),(0,r.kt)("p",null,"For convenience, the callback automatically strips whitespaces and new line characters when comparing to ",(0,r.kt)("inlineCode",{parentName:"p"},"answer_prefix_tokens"),". I.e., if ",(0,r.kt)("inlineCode",{parentName:"p"},'answer_prefix_tokens = ["The", " answer", ":"]')," then both ",(0,r.kt)("inlineCode",{parentName:"p"},'["\\nThe", " answer", ":"]')," and ",(0,r.kt)("inlineCode",{parentName:"p"},'["The", " answer", ":"]')," would be recognized a the answer prefix."),(0,r.kt)("p",null,"If you don't know the tokenized version of your answer prefix, you can determine it with the following code:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.callbacks.base import BaseCallbackHandler\n\nclass MyCallbackHandler(BaseCallbackHandler):\n    def on_llm_new_token(self, token, **kwargs) -> None:\n        # print every token on a new line\n        print(f"#{token}#")\n\nllm = OpenAI(streaming=True, callbacks=[MyCallbackHandler()])\ntools = load_tools(["wikipedia", "llm-math"], llm=llm)\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)\nagent.run("It\'s 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany.")\n')),(0,r.kt)("h3",{id:"also-streaming-the-answer-prefixes"},"Also streaming the answer prefixes"),(0,r.kt)("p",null,"When the parameter ",(0,r.kt)("inlineCode",{parentName:"p"},"stream_prefix = True")," is set, the answer prefix itself will also be streamed. This can be useful when the answer prefix itself is part of the answer. For example, when your answer is a JSON like"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},'{\n    "action": "Final answer",\n    "action_input": "Konrad Adenauer became Chancellor 74 years ago."\n}')),(0,r.kt)("p",null,"and you don't only want the action_input to be streamed, but the entire JSON."))}g.isMDXComponent=!0}}]);