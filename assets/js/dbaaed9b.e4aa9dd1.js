"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[46671],{3905:(e,t,n)=>{n.d(t,{Zo:()=>l,kt:()=>g});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},s=Object.keys(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var c=a.createContext({}),p=function(e){var t=a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},l=function(e){var t=p(e.components);return a.createElement(c.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,s=e.originalType,c=e.parentName,l=o(e,["components","mdxType","originalType","parentName"]),u=p(n),h=r,g=u["".concat(c,".").concat(h)]||u[h]||d[h]||s;return n?a.createElement(g,i(i({ref:t},l),{},{components:n})):a.createElement(g,i({ref:t},l))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var s=n.length,i=new Array(s);i[0]=h;var o={};for(var c in t)hasOwnProperty.call(t,c)&&(o[c]=t[c]);o.originalType=e,o[u]="string"==typeof e?e:r,i[1]=o;for(var p=2;p<s;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},45484:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>p});var a=n(87462),r=(n(67294),n(3905));const s={},i="Question answering over a group chat messages",o={unversionedId:"use_cases/question_answering/semantic-search-over-chat",id:"use_cases/question_answering/semantic-search-over-chat",title:"Question answering over a group chat messages",description:"In this tutorial, we are going to use Langchain + Deep Lake with GPT4 to semantically search and ask questions over a group chat.",source:"@site/docs/use_cases/question_answering/semantic-search-over-chat.md",sourceDirName:"use_cases/question_answering",slug:"/use_cases/question_answering/semantic-search-over-chat",permalink:"/langchain-docs-scratch/docs/use_cases/question_answering/semantic-search-over-chat",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/use_cases/question_answering/semantic-search-over-chat.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Question answering over documents",permalink:"/langchain-docs-scratch/docs/use_cases/question_answering/"},next:{title:"Summarization",permalink:"/langchain-docs-scratch/docs/use_cases/summarization"}},c={},p=[{value:"1. Install required packages",id:"1-install-required-packages",level:2},{value:"2. Add API keys",id:"2-add-api-keys",level:2},{value:"2. Create sample data",id:"2-create-sample-data",level:2},{value:"3. Ingest chat embeddings",id:"3-ingest-chat-embeddings",level:2},{value:"4. Ask questions",id:"4-ask-questions",level:2}],l={toc:p},u="wrapper";function d(e){let{components:t,...n}=e;return(0,r.kt)(u,(0,a.Z)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"question-answering-over-a-group-chat-messages"},"Question answering over a group chat messages"),(0,r.kt)("p",null,"In this tutorial, we are going to use Langchain + Deep Lake with GPT4 to semantically search and ask questions over a group chat."),(0,r.kt)("p",null,"View a working demo ",(0,r.kt)("a",{parentName:"p",href:"https://twitter.com/thisissukh_/status/1647223328363679745"},"here")),(0,r.kt)("h2",{id:"1-install-required-packages"},"1. Install required packages"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"python3 -m pip install --upgrade langchain deeplake openai tiktoken\n")),(0,r.kt)("h2",{id:"2-add-api-keys"},"2. Add API keys"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport getpass\nfrom langchain.document_loaders import PyPDFLoader, TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\nfrom langchain.vectorstores import DeepLake\nfrom langchain.chains import ConversationalRetrievalChain, RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n\nos.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\nos.environ['ACTIVELOOP_TOKEN'] = getpass.getpass('Activeloop Token:')\nos.environ['ACTIVELOOP_ORG'] = getpass.getpass('Activeloop Org:')\n\norg = os.environ['ACTIVELOOP_ORG']\nembeddings = OpenAIEmbeddings()\n\ndataset_path = 'hub://' + org + '/data'\n")),(0,r.kt)("h2",{id:"2-create-sample-data"},"2. Create sample data"),(0,r.kt)("p",null,"You can generate a sample group chat conversation using ChatGPT with this prompt:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Generate a group chat conversation with three friends talking about their day, referencing real places and fictional names. Make it funny and as detailed as possible.\n")),(0,r.kt)("p",null,"I've already generated such a chat in ",(0,r.kt)("inlineCode",{parentName:"p"},"messages.txt"),". We can keep it simple and use this for our example."),(0,r.kt)("h2",{id:"3-ingest-chat-embeddings"},"3. Ingest chat embeddings"),(0,r.kt)("p",null,"We load the messages in the text file, chunk and upload to ActiveLoop Vector store."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"with open(\"messages.txt\") as f:\n    state_of_the_union = f.read()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\npages = text_splitter.split_text(state_of_the_union)\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\ntexts = text_splitter.create_documents(pages)\n\nprint (texts)\n\ndataset_path = 'hub://'+org+'/data'\nembeddings = OpenAIEmbeddings()\ndb = DeepLake.from_documents(texts, embeddings, dataset_path=dataset_path, overwrite=True)\n")),(0,r.kt)("h2",{id:"4-ask-questions"},"4. Ask questions"),(0,r.kt)("p",null,"Now we can ask a question and get an answer back with a semantic search:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"db = DeepLake(dataset_path=dataset_path, read_only=True, embedding_function=embeddings)\n\nretriever = db.as_retriever()\nretriever.search_kwargs['distance_metric'] = 'cos'\nretriever.search_kwargs['k'] = 4\n\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=False)\n\n# What was the restaurant the group was talking about called?\nquery = input(\"Enter query:\")\n\n# The Hungry Lobster\nans = qa({\"query\": query})\n\nprint(ans)\n")))}d.isMDXComponent=!0}}]);