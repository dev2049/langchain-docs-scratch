"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[15611],{3905:(e,t,r)=>{r.d(t,{Zo:()=>p,kt:()=>u});var a=r(67294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var c=a.createContext({}),l=function(e){var t=a.useContext(c),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},p=function(e){var t=l(e.components);return a.createElement(c.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,c=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=l(r),h=n,u=d["".concat(c,".").concat(h)]||d[h]||m[h]||o;return r?a.createElement(u,s(s({ref:t},p),{},{components:r})):a.createElement(u,s({ref:t},p))}));function u(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,s=new Array(o);s[0]=h;var i={};for(var c in t)hasOwnProperty.call(t,c)&&(i[c]=t[c]);i.originalType=e,i[d]="string"==typeof e?e:n,s[1]=i;for(var l=2;l<o;l++)s[l]=r[l];return a.createElement.apply(null,s)}return a.createElement.apply(null,r)}h.displayName="MDXCreateElement"},58578:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>l});var a=r(87462),n=(r(67294),r(3905));const o={},s="Redis",i={unversionedId:"ecosystem/integrations/redis",id:"ecosystem/integrations/redis",title:"Redis",description:"This page covers how to use the Redis ecosystem within LangChain.",source:"@site/docs/ecosystem/integrations/redis.mdx",sourceDirName:"ecosystem/integrations",slug:"/ecosystem/integrations/redis",permalink:"/langchain-docs-scratch/docs/ecosystem/integrations/redis",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/ecosystem/integrations/redis.mdx",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Reddit",permalink:"/langchain-docs-scratch/docs/ecosystem/integrations/reddit"},next:{title:"Replicate",permalink:"/langchain-docs-scratch/docs/ecosystem/integrations/replicate"}},c={},l=[{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"Wrappers",id:"wrappers",level:2},{value:"Cache",id:"cache",level:3},{value:"Standard Cache",id:"standard-cache",level:4},{value:"Semantic Cache",id:"semantic-cache",level:4},{value:"VectorStore",id:"vectorstore",level:3},{value:"Retriever",id:"retriever",level:3},{value:"Memory",id:"memory",level:3},{value:"Vector Store Retriever Memory",id:"vector-store-retriever-memory",level:4},{value:"Chat Message History Memory",id:"chat-message-history-memory",level:4}],p={toc:l},d="wrapper";function m(e){let{components:t,...r}=e;return(0,n.kt)(d,(0,a.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"redis"},"Redis"),(0,n.kt)("p",null,"This page covers how to use the ",(0,n.kt)("a",{parentName:"p",href:"https://redis.com"},"Redis")," ecosystem within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific Redis wrappers."),(0,n.kt)("h2",{id:"installation-and-setup"},"Installation and Setup"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Install the Redis Python SDK with ",(0,n.kt)("inlineCode",{parentName:"li"},"pip install redis"))),(0,n.kt)("h2",{id:"wrappers"},"Wrappers"),(0,n.kt)("h3",{id:"cache"},"Cache"),(0,n.kt)("p",null,"The Cache wrapper allows for ",(0,n.kt)("a",{parentName:"p",href:"https://redis.io"},"Redis")," to be used as a remote, low-latency, in-memory cache for LLM prompts and responses."),(0,n.kt)("h4",{id:"standard-cache"},"Standard Cache"),(0,n.kt)("p",null,"The standard cache is the Redis bread & butter of use case in production for both ",(0,n.kt)("a",{parentName:"p",href:"https://redis.io"},"open source")," and ",(0,n.kt)("a",{parentName:"p",href:"https://redis.com"},"enterprise")," users globally."),(0,n.kt)("p",null,"To import this cache:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.cache import RedisCache\n")),(0,n.kt)("p",null,"To use this cache with your LLMs:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"import langchain\nimport redis\n\nredis_client = redis.Redis.from_url(...)\nlangchain.llm_cache = RedisCache(redis_client)\n")),(0,n.kt)("h4",{id:"semantic-cache"},"Semantic Cache"),(0,n.kt)("p",null,"Semantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends Redis as both a cache and a vectorstore."),(0,n.kt)("p",null,"To import this cache:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.cache import RedisSemanticCache\n")),(0,n.kt)("p",null,"To use this cache with your LLMs:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'import langchain\nimport redis\n\n# use any embedding provider...\nfrom tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings\n\nredis_url = "redis://localhost:6379"\n\nlangchain.llm_cache = RedisSemanticCache(\n    embedding=FakeEmbeddings(),\n    redis_url=redis_url\n)\n')),(0,n.kt)("h3",{id:"vectorstore"},"VectorStore"),(0,n.kt)("p",null,"The vectorstore wrapper turns Redis into a low-latency ",(0,n.kt)("a",{parentName:"p",href:"https://redis.com/solutions/use-cases/vector-database/"},"vector database")," for semantic search or LLM content retrieval."),(0,n.kt)("p",null,"To import this vectorstore:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.vectorstores import Redis\n")),(0,n.kt)("p",null,"For a more detailed walkthrough of the Redis vectorstore wrapper, see ",(0,n.kt)("a",{parentName:"p",href:"../modules/indexes/vectorstores/examples/redis.ipynb"},"this notebook"),"."),(0,n.kt)("h3",{id:"retriever"},"Retriever"),(0,n.kt)("p",null,"The Redis vector store retriever wrapper generalizes the vectorstore class to perform low-latency document retrieval. To create the retriever, simply call ",(0,n.kt)("inlineCode",{parentName:"p"},".as_retriever()")," on the base vectorstore class."),(0,n.kt)("h3",{id:"memory"},"Memory"),(0,n.kt)("p",null,"Redis can be used to persist LLM conversations."),(0,n.kt)("h4",{id:"vector-store-retriever-memory"},"Vector Store Retriever Memory"),(0,n.kt)("p",null,"For a more detailed walkthrough of the ",(0,n.kt)("inlineCode",{parentName:"p"},"VectorStoreRetrieverMemory")," wrapper, see ",(0,n.kt)("a",{parentName:"p",href:"../modules/memory/integrations/vectorstore_retriever_memory.ipynb"},"this notebook"),"."),(0,n.kt)("h4",{id:"chat-message-history-memory"},"Chat Message History Memory"),(0,n.kt)("p",null,"For a detailed example of Redis to cache conversation message history, see ",(0,n.kt)("a",{parentName:"p",href:"../modules/memory/examples/redis_chat_message_history.ipynb"},"this notebook"),"."))}m.isMDXComponent=!0}}]);