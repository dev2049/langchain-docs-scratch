"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[66422],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=o.createContext({}),l=function(e){var t=o.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=l(e.components);return o.createElement(c.Provider,{value:t},e.children)},h="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),h=l(n),u=a,d=h["".concat(c,".").concat(u)]||h[u]||m[u]||r;return n?o.createElement(d,i(i({ref:t},p),{},{components:n})):o.createElement(d,i({ref:t},p))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=u;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[h]="string"==typeof e?e:a,i[1]=s;for(var l=2;l<r;l++)i[l]=n[l];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}u.displayName="MDXCreateElement"},69292:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var o=n(87462),a=(n(67294),n(3905));const r={},i="Vectara Text Generation",s={unversionedId:"ecosystem/integrations/vectara/vectara_text_generation",id:"ecosystem/integrations/vectara/vectara_text_generation",title:"Vectara Text Generation",description:"This notebook is based on chatvectordb and adapted to Vectara.",source:"@site/docs/ecosystem/integrations/vectara/vectara_text_generation.md",sourceDirName:"ecosystem/integrations/vectara",slug:"/ecosystem/integrations/vectara/vectara_text_generation",permalink:"/langchain-docs-scratch/docs/ecosystem/integrations/vectara/vectara_text_generation",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/ecosystem/integrations/vectara/vectara_text_generation.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Chat Over Documents with Vectara",permalink:"/langchain-docs-scratch/docs/ecosystem/integrations/vectara/vectara_chat"},next:{title:"Vectara",permalink:"/langchain-docs-scratch/docs/ecosystem/integrations/vectara"}},c={},l=[{value:"Prepare Data",id:"prepare-data",level:2},{value:"Set Up Vector DB",id:"set-up-vector-db",level:2},{value:"Set Up LLM Chain with Custom Prompt",id:"set-up-llm-chain-with-custom-prompt",level:2},{value:"Generate Text",id:"generate-text",level:2}],p=(h="CodeOutputBlock",function(e){return console.warn("Component "+h+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var h;const m={toc:l},u="wrapper";function d(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,o.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"vectara-text-generation"},"Vectara Text Generation"),(0,a.kt)("p",null,"This notebook is based on ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/hwchase17/langchain/blob/master/docs/modules/chains/index_examples/question_answering.ipynb"},"chat_vector_db")," and adapted to Vectara."),(0,a.kt)("h2",{id:"prepare-data"},"Prepare Data"),(0,a.kt)("p",null,"First, we prepare the data. For this example, we fetch a documentation site that consists of markdown files hosted on Github and split them into small enough Documents."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.llms import OpenAI\nfrom langchain.docstore.document import Document\nimport requests\nfrom langchain.vectorstores import Vectara\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nimport pathlib\nimport subprocess\nimport tempfile\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'def get_github_docs(repo_owner, repo_name):\n    with tempfile.TemporaryDirectory() as d:\n        subprocess.check_call(\n            f"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .",\n            cwd=d,\n            shell=True,\n        )\n        git_sha = (\n            subprocess.check_output("git rev-parse HEAD", shell=True, cwd=d)\n            .decode("utf-8")\n            .strip()\n        )\n        repo_path = pathlib.Path(d)\n        markdown_files = list(repo_path.glob("*/*.md")) + list(\n            repo_path.glob("*/*.mdx")\n        )\n        for markdown_file in markdown_files:\n            with open(markdown_file, "r") as f:\n                relative_path = markdown_file.relative_to(repo_path)\n                github_url = f"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}"\n                yield Document(page_content=f.read(), metadata={"source": github_url})\n\nsources = get_github_docs("yirenlu92", "deno-manual-forked")\n\nsource_chunks = []\nsplitter = CharacterTextSplitter(separator=" ", chunk_size=1024, chunk_overlap=0)\nfor source in sources:\n    for chunk in splitter.split_text(source.page_content):\n        source_chunks.append(chunk)\n')),(0,a.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    Cloning into '.'...\n"))),(0,a.kt)("h2",{id:"set-up-vector-db"},"Set Up Vector DB"),(0,a.kt)("p",null,"Now that we have the documentation content in chunks, let's put all this information in a vector index for easy retrieval."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import os\nsearch_index = Vectara.from_texts(source_chunks, embedding=None)\n")),(0,a.kt)("h2",{id:"set-up-llm-chain-with-custom-prompt"},"Set Up LLM Chain with Custom Prompt"),(0,a.kt)("p",null,"Next, let's set up a simple LLM chain but give it a custom prompt for blog post generation. Note that the custom prompt is parameterized and takes two inputs: ",(0,a.kt)("inlineCode",{parentName:"p"},"context"),", which will be the documents fetched from the vector search, and ",(0,a.kt)("inlineCode",{parentName:"p"},"topic"),", which is given by the user."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chains import LLMChain\nprompt_template = """Use the context below to write a 400 word blog post about the topic below:\n    Context: {context}\n    Topic: {topic}\n    Blog post:"""\n\nPROMPT = PromptTemplate(\n    template=prompt_template, input_variables=["context", "topic"]\n)\n\nllm = OpenAI(openai_api_key=os.environ[\'OPENAI_API_KEY\'], temperature=0)\n\nchain = LLMChain(llm=llm, prompt=PROMPT)\n')),(0,a.kt)("h2",{id:"generate-text"},"Generate Text"),(0,a.kt)("p",null,"Finally, we write a function to apply our inputs to the chain. The function takes an input parameter ",(0,a.kt)("inlineCode",{parentName:"p"},"topic"),". We find the documents in the vector index that correspond to that ",(0,a.kt)("inlineCode",{parentName:"p"},"topic"),", and use them as additional context in our simple LLM chain."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'def generate_blog_post(topic):\n    docs = search_index.similarity_search(topic, k=4)\n    inputs = [{"context": doc.page_content, "topic": topic} for doc in docs]\n    print(chain.apply(inputs))\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'generate_blog_post("environment variables")\n')),(0,a.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    [{'text': '\\n\\nEnvironment variables are an essential part of any development workflow. They provide a way to store and access information that is specific to the environment in which the code is running. This can be especially useful when working with different versions of a language or framework, or when running code on different machines.\\n\\nThe Deno CLI tasks extension provides a way to easily manage environment variables when running Deno commands. This extension provides a task definition for allowing you to create tasks that execute the `deno` CLI from within the editor. The template for the Deno CLI tasks has the following interface, which can be configured in a `tasks.json` within your workspace:\\n\\nThe task definition includes the `type` field, which should be set to `deno`, and the `command` field, which is the `deno` command to run (e.g. `run`, `test`, `cache`, etc.). Additionally, you can specify additional arguments to pass on the command line, the current working directory to execute the command, and any environment variables.\\n\\nUsing environment variables with the Deno CLI tasks extension is a great way to ensure that your code is running in the correct environment. For example, if you are running a test suite,'}, {'text': '\\n\\nEnvironment variables are an important part of any programming language, and they can be used to store and access data in a variety of ways. In this blog post, we\\'ll be taking a look at environment variables specifically for the shell.\\n\\nShell variables are similar to environment variables, but they won\\'t be exported to spawned commands. They are defined with the following syntax:\\n\\n```sh\\nVAR_NAME=value\\n```\\n\\nShell variables can be used to store and access data in a variety of ways. For example, you can use them to store values that you want to re-use, but don\\'t want to be available in any spawned processes.\\n\\nFor example, if you wanted to store a value and then use it in a command, you could do something like this:\\n\\n```sh\\nVAR=hello && echo $VAR && deno eval \"console.log(\\'Deno: \\' + Deno.env.get(\\'VAR\\'))\"\\n```\\n\\nThis would output the following:\\n\\n```\\nhello\\nDeno: undefined\\n```\\n\\nAs you can see, the value stored in the shell variable is not available in the spawned process.\\n\\n'}, {'text': '\\n\\nWhen it comes to developing applications, environment variables are an essential part of the process. Environment variables are used to store information that can be used by applications and scripts to customize their behavior. This is especially important when it comes to developing applications with Deno, as there are several environment variables that can impact the behavior of Deno.\\n\\nThe most important environment variable for Deno is `DENO_AUTH_TOKENS`. This environment variable is used to store authentication tokens that are used to access remote resources. This is especially important when it comes to accessing remote APIs or databases. Without the proper authentication tokens, Deno will not be able to access the remote resources.\\n\\nAnother important environment variable for Deno is `DENO_DIR`. This environment variable is used to store the directory where Deno will store its files. This includes the Deno executable, the Deno cache, and the Deno configuration files. By setting this environment variable, you can ensure that Deno will always be able to find the files it needs.\\n\\nFinally, there is the `DENO_PLUGINS` environment variable. This environment variable is used to store the list of plugins that Deno will use. This is important for customizing the'}, {'text': '\\n\\nEnvironment variables are a great way to store and access sensitive information in your Deno applications. Deno offers built-in support for environment variables with `Deno.env`, and you can also use a `.env` file to store and access environment variables. In this blog post, we\\'ll explore both of these options and how to use them in your Deno applications.\\n\\n## Built-in `Deno.env`\\n\\nThe Deno runtime offers built-in support for environment variables with [`Deno.env`](https://deno.land/api@v1.25.3?s=Deno.env). `Deno.env` has getter and setter methods. Here is example usage:\\n\\n```ts\\nDeno.env.set(\"FIREBASE_API_KEY\", \"examplekey123\");\\nDeno.env.set(\"FIREBASE_AUTH_DOMAIN\", \"firebasedomain.com\");\\n\\nconsole.log(Deno.env.get(\"FIREBASE_API_KEY\")); // examplekey123\\nconsole.log(Deno.env.get(\"FIREBASE_AUTH_'}]\n"))))}d.isMDXComponent=!0}}]);