"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[29360],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>m});var a=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function c(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var s=a.createContext({}),i=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},d=function(e){var n=i(e.components);return a.createElement(s.Provider,{value:n},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},h=a.forwardRef((function(e,n){var t=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,d=c(e,["components","mdxType","originalType","parentName"]),u=i(t),h=o,m=u["".concat(s,".").concat(h)]||u[h]||p[h]||r;return t?a.createElement(m,l(l({ref:n},d),{},{components:t})):a.createElement(m,l({ref:n},d))}));function m(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var r=t.length,l=new Array(r);l[0]=h;var c={};for(var s in n)hasOwnProperty.call(n,s)&&(c[s]=n[s]);c.originalType=e,c[u]="string"==typeof e?e:o,l[1]=c;for(var i=2;i<r;i++)l[i]=t[i];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}h.displayName="MDXCreateElement"},52387:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>c,toc:()=>i});var a=t(87462),o=(t(67294),t(3905));const r={},l="Async callbacks",c={unversionedId:"modules/callbacks/how_to/async_callbacks",id:"modules/callbacks/how_to/async_callbacks",title:"Async callbacks",description:"If you are planning to use the async API, it is recommended to use AsyncCallbackHandler to avoid blocking the runloop.",source:"@site/docs/modules/callbacks/how_to/async_callbacks.md",sourceDirName:"modules/callbacks/how_to",slug:"/modules/callbacks/how_to/async_callbacks",permalink:"/langchain-docs-scratch/docs/modules/callbacks/how_to/async_callbacks",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/callbacks/how_to/async_callbacks.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Callbacks",permalink:"/langchain-docs-scratch/docs/modules/callbacks/"},next:{title:"Custom callback handlers",permalink:"/langchain-docs-scratch/docs/modules/callbacks/how_to/custom_callbacks"}},s={},i=[],d=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var u;const p={toc:i},h="wrapper";function m(e){let{components:n,...t}=e;return(0,o.kt)(h,(0,a.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"async-callbacks"},"Async callbacks"),(0,o.kt)("p",null,"If you are planning to use the async API, it is recommended to use ",(0,o.kt)("inlineCode",{parentName:"p"},"AsyncCallbackHandler")," to avoid blocking the runloop. "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Advanced")," if you use a sync ",(0,o.kt)("inlineCode",{parentName:"p"},"CallbackHandler")," while using an async method to run your llm/chain/tool/agent, it will still work. However, under the hood, it will be called with ",(0,o.kt)("a",{parentName:"p",href:"https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor"},(0,o.kt)("inlineCode",{parentName:"a"},"run_in_executor"))," which can cause issues if your ",(0,o.kt)("inlineCode",{parentName:"p"},"CallbackHandler")," is not thread-safe."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import asyncio\nfrom typing import Any, Dict, List\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import LLMResult, HumanMessage\nfrom langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n\n\nclass MyCustomSyncHandler(BaseCallbackHandler):\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        print(f"Sync handler being called in a `thread_pool_executor`: token: {token}")\n\nclass MyCustomAsyncHandler(AsyncCallbackHandler):\n    """Async callback handler that can be used to handle callbacks from langchain."""\n\n    async def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> None:\n        """Run when chain starts running."""\n        print("zzzz....")\n        await asyncio.sleep(0.3)\n        class_name = serialized["name"]\n        print("Hi! I just woke up. Your llm is starting")\n\n    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        """Run when chain ends running."""\n        print("zzzz....")\n        await asyncio.sleep(0.3)\n        print("Hi! I just woke up. Your llm is ending")\n\n# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n# Additionally, we pass in a list with our custom handler\nchat = ChatOpenAI(max_tokens=25, streaming=True, callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()])\n\nawait chat.agenerate([[HumanMessage(content="Tell me a joke")]])\n')),(0,o.kt)(d,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    zzzz....\n    Hi! I just woke up. Your llm is starting\n    Sync handler being called in a `thread_pool_executor`: token: \n    Sync handler being called in a `thread_pool_executor`: token: Why\n    Sync handler being called in a `thread_pool_executor`: token:  don\n    Sync handler being called in a `thread_pool_executor`: token: 't\n    Sync handler being called in a `thread_pool_executor`: token:  scientists\n    Sync handler being called in a `thread_pool_executor`: token:  trust\n    Sync handler being called in a `thread_pool_executor`: token:  atoms\n    Sync handler being called in a `thread_pool_executor`: token: ?\n    Sync handler being called in a `thread_pool_executor`: token:  \n    \n    \n    Sync handler being called in a `thread_pool_executor`: token: Because\n    Sync handler being called in a `thread_pool_executor`: token:  they\n    Sync handler being called in a `thread_pool_executor`: token:  make\n    Sync handler being called in a `thread_pool_executor`: token:  up\n    Sync handler being called in a `thread_pool_executor`: token:  everything\n    Sync handler being called in a `thread_pool_executor`: token: .\n    Sync handler being called in a `thread_pool_executor`: token: \n    zzzz....\n    Hi! I just woke up. Your llm is ending\n\n\n\n\n\n    LLMResult(generations=[[ChatGeneration(text=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything.\", generation_info=None, message=AIMessage(content=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'})\n"))))}m.isMDXComponent=!0}}]);