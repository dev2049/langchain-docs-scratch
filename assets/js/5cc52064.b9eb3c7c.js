"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2885],{3905:(e,n,t)=>{t.d(n,{Zo:()=>m,kt:()=>d});var o=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,o,r=function(e,n){if(null==e)return{};var t,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var u=o.createContext({}),l=function(e){var n=o.useContext(u),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},m=function(e){var n=l(e.components);return o.createElement(u.Provider,{value:n},e.children)},c="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},h=o.forwardRef((function(e,n){var t=e.components,r=e.mdxType,a=e.originalType,u=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),c=l(t),h=r,d=c["".concat(u,".").concat(h)]||c[h]||p[h]||a;return t?o.createElement(d,i(i({ref:n},m),{},{components:t})):o.createElement(d,i({ref:n},m))}));function d(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var a=t.length,i=new Array(a);i[0]=h;var s={};for(var u in n)hasOwnProperty.call(n,u)&&(s[u]=n[u]);s.originalType=e,s[c]="string"==typeof e?e:r,i[1]=s;for(var l=2;l<a;l++)i[l]=t[l];return o.createElement.apply(null,i)}return o.createElement.apply(null,t)}h.displayName="MDXCreateElement"},25643:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>i,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var o=t(87462),r=(t(67294),t(3905));const a={},i="ConversationTokenBufferMemory",s={unversionedId:"modules/memory/how_to/token_buffer",id:"modules/memory/how_to/token_buffer",title:"ConversationTokenBufferMemory",description:"ConversationTokenBufferMemory keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions.",source:"@site/docs/modules/memory/how_to/token_buffer.md",sourceDirName:"modules/memory/how_to",slug:"/modules/memory/how_to/token_buffer",permalink:"/langchain-docs-scratch/docs/modules/memory/how_to/token_buffer",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/memory/how_to/token_buffer.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"ConversationSummaryBufferMemory",permalink:"/langchain-docs-scratch/docs/modules/memory/how_to/summary_buffer"},next:{title:"Vector store-backed memory",permalink:"/langchain-docs-scratch/docs/modules/memory/how_to/vectorstore_retriever_memory"}},u={},l=[{value:"Using in a chain",id:"using-in-a-chain",level:2}],m=(c="CodeOutputBlock",function(e){return console.warn("Component "+c+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var c;const p={toc:l},h="wrapper";function d(e){let{components:n,...t}=e;return(0,r.kt)(h,(0,o.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"conversationtokenbuffermemory"},"ConversationTokenBufferMemory"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"ConversationTokenBufferMemory")," keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions."),(0,r.kt)("p",null,"Let's first walk through how to use the utilities"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.memory import ConversationTokenBufferMemory\nfrom langchain.llms import OpenAI\nllm = OpenAI()\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\nmemory.save_context({"input": "hi"}, {"output": "whats up"})\nmemory.save_context({"input": "not much you"}, {"output": "not much"})\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"memory.load_memory_variables({})\n")),(0,r.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    {'history': 'Human: not much you\\nAI: not much'}\n"))),(0,r.kt)("p",null,"We can also get the history as a list of messages (this is useful if you are using this with a chat model)."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10, return_messages=True)\nmemory.save_context({"input": "hi"}, {"output": "whats up"})\nmemory.save_context({"input": "not much you"}, {"output": "not much"})\n')),(0,r.kt)("h2",{id:"using-in-a-chain"},"Using in a chain"),(0,r.kt)("p",null,"Let's walk through an example, again setting ",(0,r.kt)("inlineCode",{parentName:"p"},"verbose=True")," so we can see the prompt."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chains import ConversationChain\nconversation_with_summary = ConversationChain(\n    llm=llm, \n    # We set a very low max_token_limit for the purposes of testing.\n    memory=ConversationTokenBufferMemory(llm=OpenAI(), max_token_limit=60),\n    verbose=True\n)\nconversation_with_summary.predict(input="Hi, what\'s up?")\n')),(0,r.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    \n    \n    > Entering new ConversationChain chain...\n    Prompt after formatting:\n    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n    \n    Current conversation:\n    \n    Human: Hi, what's up?\n    AI:\n    \n    > Finished chain.\n\n\n\n\n\n    \" Hi there! I'm doing great, just enjoying the day. How about you?\"\n"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'conversation_with_summary.predict(input="Just working on writing some documentation!")\n')),(0,r.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    \n    \n    > Entering new ConversationChain chain...\n    Prompt after formatting:\n    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n    \n    Current conversation:\n    Human: Hi, what's up?\n    AI:  Hi there! I'm doing great, just enjoying the day. How about you?\n    Human: Just working on writing some documentation!\n    AI:\n    \n    > Finished chain.\n\n\n\n\n\n    ' Sounds like a productive day! What kind of documentation are you writing?'\n"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'conversation_with_summary.predict(input="For LangChain! Have you heard of it?")\n')),(0,r.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    \n    \n    > Entering new ConversationChain chain...\n    Prompt after formatting:\n    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n    \n    Current conversation:\n    Human: Hi, what's up?\n    AI:  Hi there! I'm doing great, just enjoying the day. How about you?\n    Human: Just working on writing some documentation!\n    AI:  Sounds like a productive day! What kind of documentation are you writing?\n    Human: For LangChain! Have you heard of it?\n    AI:\n    \n    > Finished chain.\n\n\n\n\n\n    \" Yes, I have heard of LangChain! It is a decentralized language-learning platform that connects native speakers and learners in real time. Is that the documentation you're writing about?\"\n"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# We can see here that the buffer is updated\nconversation_with_summary.predict(input="Haha nope, although a lot of people confuse it for that")\n')),(0,r.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    \n    \n    > Entering new ConversationChain chain...\n    Prompt after formatting:\n    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n    \n    Current conversation:\n    Human: For LangChain! Have you heard of it?\n    AI:  Yes, I have heard of LangChain! It is a decentralized language-learning platform that connects native speakers and learners in real time. Is that the documentation you're writing about?\n    Human: Haha nope, although a lot of people confuse it for that\n    AI:\n    \n    > Finished chain.\n\n\n\n\n\n    \" Oh, I see. Is there another language learning platform you're referring to?\"\n"))))}d.isMDXComponent=!0}}]);