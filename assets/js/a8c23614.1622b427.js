"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[87494],{3905:(n,e,t)=>{t.d(e,{Zo:()=>u,kt:()=>g});var a=t(67294);function r(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function o(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(n);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,a)}return t}function l(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?o(Object(t),!0).forEach((function(e){r(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}function s(n,e){if(null==n)return{};var t,a,r=function(n,e){if(null==n)return{};var t,a,r={},o=Object.keys(n);for(a=0;a<o.length;a++)t=o[a],e.indexOf(t)>=0||(r[t]=n[t]);return r}(n,e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(n);for(a=0;a<o.length;a++)t=o[a],e.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(n,t)&&(r[t]=n[t])}return r}var i=a.createContext({}),c=function(n){var e=a.useContext(i),t=e;return n&&(t="function"==typeof n?n(e):l(l({},e),n)),t},u=function(n){var e=c(n.components);return a.createElement(i.Provider,{value:e},n.children)},p="mdxType",m={inlineCode:"code",wrapper:function(n){var e=n.children;return a.createElement(a.Fragment,{},e)}},h=a.forwardRef((function(n,e){var t=n.components,r=n.mdxType,o=n.originalType,i=n.parentName,u=s(n,["components","mdxType","originalType","parentName"]),p=c(t),h=r,g=p["".concat(i,".").concat(h)]||p[h]||m[h]||o;return t?a.createElement(g,l(l({ref:e},u),{},{components:t})):a.createElement(g,l({ref:e},u))}));function g(n,e){var t=arguments,r=e&&e.mdxType;if("string"==typeof n||r){var o=t.length,l=new Array(o);l[0]=h;var s={};for(var i in e)hasOwnProperty.call(e,i)&&(s[i]=e[i]);s.originalType=n,s[p]="string"==typeof n?n:r,l[1]=s;for(var c=2;c<o;c++)l[c]=t[c];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}h.displayName="MDXCreateElement"},72860:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>i,contentTitle:()=>l,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var a=t(87462),r=(t(67294),t(3905));const o={},l="Custom chain",s={unversionedId:"modules/chains/how_to/custom_chain",id:"modules/chains/how_to/custom_chain",title:"Custom chain",description:"To implement your own custom chain you can subclass Chain and implement the following methods:",source:"@site/docs/modules/chains/how_to/custom_chain.md",sourceDirName:"modules/chains/how_to",slug:"/modules/chains/how_to/custom_chain",permalink:"/langchain-docs-scratch/docs/modules/chains/how_to/custom_chain",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/chains/how_to/custom_chain.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Async API",permalink:"/langchain-docs-scratch/docs/modules/chains/how_to/async_chain"},next:{title:"LLM chain",permalink:"/langchain-docs-scratch/docs/modules/chains/how_to/llm_chain"}},i={},c=[],u=(p="CodeOutputBlock",function(n){return console.warn("Component "+p+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",n)});var p;const m={toc:c},h="wrapper";function g(n){let{components:e,...t}=n;return(0,r.kt)(h,(0,a.Z)({},m,t,{components:e,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"custom-chain"},"Custom chain"),(0,r.kt)("p",null,"To implement your own custom chain you can subclass ",(0,r.kt)("inlineCode",{parentName:"p"},"Chain")," and implement the following methods:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from __future__ import annotations\n\nfrom typing import Any, Dict, List, Optional\n\nfrom pydantic import Extra\n\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.prompts.base import BasePromptTemplate\n\n\nclass MyCustomChain(Chain):\n    """\n    An example of a custom chain.\n    """\n\n    prompt: BasePromptTemplate\n    """Prompt object to use."""\n    llm: BaseLanguageModel\n    output_key: str = "text"  #: :meta private:\n\n    class Config:\n        """Configuration for this pydantic object."""\n\n        extra = Extra.forbid\n        arbitrary_types_allowed = True\n\n    @property\n    def input_keys(self) -> List[str]:\n        """Will be whatever keys the prompt expects.\n\n        :meta private:\n        """\n        return self.prompt.input_variables\n\n    @property\n    def output_keys(self) -> List[str]:\n        """Will always return text key.\n\n        :meta private:\n        """\n        return [self.output_key]\n\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        # Your custom chain logic goes here\n        # This is just an example that mimics LLMChain\n        prompt_value = self.prompt.format_prompt(**inputs)\n        \n        # Whenever you call a language model, or another chain, you should pass\n        # a callback manager to it. This allows the inner run to be tracked by\n        # any callbacks that are registered on the outer run.\n        # You can always obtain a callback manager for this by calling\n        # `run_manager.get_child()` as shown below.\n        response = self.llm.generate_prompt(\n            [prompt_value],\n            callbacks=run_manager.get_child() if run_manager else None\n        )\n\n        # If you want to log something about this run, you can do so by calling\n        # methods on the `run_manager`, as shown below. This will trigger any\n        # callbacks that are registered for that event.\n        if run_manager:\n            run_manager.on_text("Log something about this run")\n        \n        return {self.output_key: response.generations[0][0].text}\n\n    async def _acall(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        # Your custom chain logic goes here\n        # This is just an example that mimics LLMChain\n        prompt_value = self.prompt.format_prompt(**inputs)\n        \n        # Whenever you call a language model, or another chain, you should pass\n        # a callback manager to it. This allows the inner run to be tracked by\n        # any callbacks that are registered on the outer run.\n        # You can always obtain a callback manager for this by calling\n        # `run_manager.get_child()` as shown below.\n        response = await self.llm.agenerate_prompt(\n            [prompt_value],\n            callbacks=run_manager.get_child() if run_manager else None\n        )\n\n        # If you want to log something about this run, you can do so by calling\n        # methods on the `run_manager`, as shown below. This will trigger any\n        # callbacks that are registered for that event.\n        if run_manager:\n            await run_manager.on_text("Log something about this run")\n        \n        return {self.output_key: response.generations[0][0].text}\n\n    @property\n    def _chain_type(self) -> str:\n        return "my_custom_chain"\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.callbacks.stdout import StdOutCallbackHandler\nfrom langchain.chat_models.openai import ChatOpenAI\nfrom langchain.prompts.prompt import PromptTemplate\n\n\nchain = MyCustomChain(\n    prompt=PromptTemplate.from_template('tell us a joke about {topic}'),\n    llm=ChatOpenAI()\n)\n\nchain.run({'topic': 'callbacks'}, callbacks=[StdOutCallbackHandler()])\n")),(0,r.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    \n    \n    > Entering new MyCustomChain chain...\n    Log something about this run\n    > Finished chain.\n\n\n\n\n\n    'Why did the callback function feel lonely? Because it was always waiting for someone to call it back!'\n"))))}g.isMDXComponent=!0}}]);