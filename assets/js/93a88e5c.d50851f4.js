"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[66840],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>h});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function c(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),u=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):c(c({},t),e)),n},m=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},i="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),i=u(n),p=o,h=i["".concat(s,".").concat(p)]||i[p]||d[p]||r;return n?a.createElement(h,c(c({ref:t},m),{},{components:n})):a.createElement(h,c({ref:t},m))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,c=new Array(r);c[0]=p;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[i]="string"==typeof e?e:o,c[1]=l;for(var u=2;u<r;u++)c[u]=n[u];return a.createElement.apply(null,c)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},5500:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>u});var a=n(87462),o=(n(67294),n(3905));const r={},c="Custom callback handlers",l={unversionedId:"modules/callbacks/how_to/custom_callbacks",id:"modules/callbacks/how_to/custom_callbacks",title:"Custom callback handlers",description:"You can create a custom handler to set on the object as well. In the example below, we'll implement streaming with a custom handler.",source:"@site/docs/modules/callbacks/how_to/custom_callbacks.md",sourceDirName:"modules/callbacks/how_to",slug:"/modules/callbacks/how_to/custom_callbacks",permalink:"/langchain-docs-scratch/docs/modules/callbacks/how_to/custom_callbacks",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/callbacks/how_to/custom_callbacks.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Async callbacks",permalink:"/langchain-docs-scratch/docs/modules/callbacks/how_to/async_callbacks"},next:{title:"Callbacks for custom chains",permalink:"/langchain-docs-scratch/docs/modules/callbacks/how_to/custom_chain"}},s={},u=[],m=(i="CodeOutputBlock",function(e){return console.warn("Component "+i+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var i;const d={toc:u},p="wrapper";function h(e){let{components:t,...n}=e;return(0,o.kt)(p,(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"custom-callback-handlers"},"Custom callback handlers"),(0,o.kt)("p",null,"You can create a custom handler to set on the object as well. In the example below, we'll implement streaming with a custom handler."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nclass MyCustomHandler(BaseCallbackHandler):\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        print(f"My custom handler, token: {token}")\n\n# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n# Additionally, we pass in a list with our custom handler\nchat = ChatOpenAI(max_tokens=25, streaming=True, callbacks=[MyCustomHandler()])\n\nchat([HumanMessage(content="Tell me a joke")])\n')),(0,o.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    My custom handler, token: \n    My custom handler, token: Why\n    My custom handler, token:  don\n    My custom handler, token: 't\n    My custom handler, token:  scientists\n    My custom handler, token:  trust\n    My custom handler, token:  atoms\n    My custom handler, token: ?\n    My custom handler, token:  \n    \n    \n    My custom handler, token: Because\n    My custom handler, token:  they\n    My custom handler, token:  make\n    My custom handler, token:  up\n    My custom handler, token:  everything\n    My custom handler, token: .\n    My custom handler, token: \n\n\n\n\n\n    AIMessage(content=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything.\", additional_kwargs={}, example=False)\n"))))}h.isMDXComponent=!0}}]);