"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[23063],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>g});var o=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=o.createContext({}),c=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=c(e.components);return o.createElement(s.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),m=c(n),d=r,g=m["".concat(s,".").concat(d)]||m[d]||u[d]||a;return n?o.createElement(g,l(l({ref:t},p),{},{components:n})):o.createElement(g,l({ref:t},p))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,l=new Array(a);l[0]=d;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[m]="string"==typeof e?e:r,l[1]=i;for(var c=2;c<a;c++)l[c]=n[c];return o.createElement.apply(null,l)}return o.createElement.apply(null,n)}d.displayName="MDXCreateElement"},96749:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>g,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var o=n(87462),r=(n(67294),n(3905));const a={},l="Tracking token usage",i={unversionedId:"modules/model_io/models/llms/how_to/token_usage_tracking",id:"modules/model_io/models/llms/how_to/token_usage_tracking",title:"Tracking token usage",description:"This notebook goes over how to track your token usage for specific calls. It is currently only implemented for the OpenAI API.",source:"@site/docs/modules/model_io/models/llms/how_to/token_usage_tracking.md",sourceDirName:"modules/model_io/models/llms/how_to",slug:"/modules/model_io/models/llms/how_to/token_usage_tracking",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/how_to/token_usage_tracking",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/model_io/models/llms/how_to/token_usage_tracking.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Streaming",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/how_to/streaming_llm"},next:{title:"AI21",permalink:"/langchain-docs-scratch/docs/modules/model_io/models/llms/integrations/ai21"}},s={},c=[],p=(m="CodeOutputBlock",function(e){return console.warn("Component "+m+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var m;const u={toc:c},d="wrapper";function g(e){let{components:t,...n}=e;return(0,r.kt)(d,(0,o.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"tracking-token-usage"},"Tracking token usage"),(0,r.kt)("p",null,"This notebook goes over how to track your token usage for specific calls. It is currently only implemented for the OpenAI API."),(0,r.kt)("p",null,"Let's first look at an extremely simple example of tracking token usage for a single LLM call."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.llms import OpenAI\nfrom langchain.callbacks import get_openai_callback\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'llm = OpenAI(model_name="text-davinci-002", n=2, best_of=2)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'with get_openai_callback() as cb:\n    result = llm("Tell me a joke")\n    print(cb)\n')),(0,r.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    Tokens Used: 42\n        Prompt Tokens: 4\n        Completion Tokens: 38\n    Successful Requests: 1\n    Total Cost (USD): $0.00084\n"))),(0,r.kt)("p",null,"Anything inside the context manager will get tracked. Here's an example of using it to track multiple calls in sequence."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'with get_openai_callback() as cb:\n    result = llm("Tell me a joke")\n    result2 = llm("Tell me a joke")\n    print(cb.total_tokens)\n')),(0,r.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    91\n"))),(0,r.kt)("p",null,"If a chain or agent with multiple steps in it is used, it will track all those steps."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0)\ntools = load_tools(["serpapi", "llm-math"], llm=llm)\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'with get_openai_callback() as cb:\n    response = agent.run("Who is Olivia Wilde\'s boyfriend? What is his current age raised to the 0.23 power?")\n    print(f"Total Tokens: {cb.total_tokens}")\n    print(f"Prompt Tokens: {cb.prompt_tokens}")\n    print(f"Completion Tokens: {cb.completion_tokens}")\n    print(f"Total Cost (USD): ${cb.total_cost}")\n')),(0,r.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    \n    \n    > Entering new AgentExecutor chain...\n     I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n    Action: Search\n    Action Input: \"Olivia Wilde boyfriend\"\n    Observation: Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.\n    Thought: I need to find out Harry Styles' age.\n    Action: Search\n    Action Input: \"Harry Styles age\"\n    Observation: 29 years\n    Thought: I need to calculate 29 raised to the 0.23 power.\n    Action: Calculator\n    Action Input: 29^0.23\n    Observation: Answer: 2.169459462491557\n    \n    Thought: I now know the final answer.\n    Final Answer: Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557.\n    \n    > Finished chain.\n    Total Tokens: 1506\n    Prompt Tokens: 1350\n    Completion Tokens: 156\n    Total Cost (USD): $0.03012\n"))))}g.isMDXComponent=!0}}]);