"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3536],{3905:(n,e,t)=>{t.d(e,{Zo:()=>_,kt:()=>u});var o=t(67294);function a(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function l(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(n);e&&(o=o.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,o)}return t}function r(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?l(Object(t),!0).forEach((function(e){a(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}function s(n,e){if(null==n)return{};var t,o,a=function(n,e){if(null==n)return{};var t,o,a={},l=Object.keys(n);for(o=0;o<l.length;o++)t=l[o],e.indexOf(t)>=0||(a[t]=n[t]);return a}(n,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(n);for(o=0;o<l.length;o++)t=l[o],e.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(n,t)&&(a[t]=n[t])}return a}var c=o.createContext({}),i=function(n){var e=o.useContext(c),t=e;return n&&(t="function"==typeof n?n(e):r(r({},e),n)),t},_=function(n){var e=i(n.components);return o.createElement(c.Provider,{value:e},n.children)},p="mdxType",k={inlineCode:"code",wrapper:function(n){var e=n.children;return o.createElement(o.Fragment,{},e)}},d=o.forwardRef((function(n,e){var t=n.components,a=n.mdxType,l=n.originalType,c=n.parentName,_=s(n,["components","mdxType","originalType","parentName"]),p=i(t),d=a,u=p["".concat(c,".").concat(d)]||p[d]||k[d]||l;return t?o.createElement(u,r(r({ref:e},_),{},{components:t})):o.createElement(u,r({ref:e},_))}));function u(n,e){var t=arguments,a=e&&e.mdxType;if("string"==typeof n||a){var l=t.length,r=new Array(l);r[0]=d;var s={};for(var c in e)hasOwnProperty.call(e,c)&&(s[c]=e[c]);s.originalType=n,s[p]="string"==typeof n?n:a,r[1]=s;for(var i=2;i<l;i++)r[i]=t[i];return o.createElement.apply(null,r)}return o.createElement.apply(null,t)}d.displayName="MDXCreateElement"},87885:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>i});var o=t(87462),a=(t(67294),t(3905));const l={},r="Multiple callback handlers",s={unversionedId:"modules/callbacks/how_to/multiple_callbacks",id:"modules/callbacks/how_to/multiple_callbacks",title:"Multiple callback handlers",description:"In the previous examples, we passed in callback handlers upon creation of an object by using callbacks=. In this case, the callbacks will be scoped to that particular object.",source:"@site/docs/modules/callbacks/how_to/multiple_callbacks.md",sourceDirName:"modules/callbacks/how_to",slug:"/modules/callbacks/how_to/multiple_callbacks",permalink:"/langchain-docs-scratch/docs/modules/callbacks/how_to/multiple_callbacks",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/edit/main/docs/docs/modules/callbacks/how_to/multiple_callbacks.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Logging to file",permalink:"/langchain-docs-scratch/docs/modules/callbacks/how_to/filecallbackhandler"},next:{title:"Token counting",permalink:"/langchain-docs-scratch/docs/modules/callbacks/how_to/token_counting"}},c={},i=[],_=(p="CodeOutputBlock",function(n){return console.warn("Component "+p+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",n)});var p;const k={toc:i},d="wrapper";function u(n){let{components:e,...t}=n;return(0,a.kt)(d,(0,o.Z)({},k,t,{components:e,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"multiple-callback-handlers"},"Multiple callback handlers"),(0,a.kt)("p",null,"In the previous examples, we passed in callback handlers upon creation of an object by using ",(0,a.kt)("inlineCode",{parentName:"p"},"callbacks="),". In this case, the callbacks will be scoped to that particular object. "),(0,a.kt)("p",null,"However, in many cases, it is advantageous to pass in handlers instead when running the object. When we pass through ",(0,a.kt)("inlineCode",{parentName:"p"},"CallbackHandlers")," using the ",(0,a.kt)("inlineCode",{parentName:"p"},"callbacks")," keyword arg when executing an run, those callbacks will be issued by all nested objects involved in the execution. For example, when a handler is passed through to an ",(0,a.kt)("inlineCode",{parentName:"p"},"Agent"),", it will be used for all callbacks related to the agent and all the objects involved in the agent's execution, in this case, the ",(0,a.kt)("inlineCode",{parentName:"p"},"Tools"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"LLMChain"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"LLM"),"."),(0,a.kt)("p",null,"This prevents us from having to manually attach the handlers to each individual nested object."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from typing import Dict, Union, Any, List\n\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.callbacks import tracing_enabled\nfrom langchain.llms import OpenAI\n\n# First, define custom callback handler implementations\nclass MyCustomHandlerOne(BaseCallbackHandler):\n    def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> Any:\n        print(f"on_llm_start {serialized[\'name\']}")\n\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n        print(f"on_new_token {token}")\n\n    def on_llm_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> Any:\n        """Run when LLM errors."""\n\n    def on_chain_start(\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n    ) -> Any:\n        print(f"on_chain_start {serialized[\'name\']}")\n\n    def on_tool_start(\n        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n    ) -> Any:\n        print(f"on_tool_start {serialized[\'name\']}")\n\n    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n        print(f"on_agent_action {action}")\n\nclass MyCustomHandlerTwo(BaseCallbackHandler):\n    def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> Any:\n        print(f"on_llm_start (I\'m the second handler!!) {serialized[\'name\']}")\n\n# Instantiate the handlers\nhandler1 = MyCustomHandlerOne()\nhandler2 = MyCustomHandlerTwo()\n\n# Setup the agent. Only the `llm` will issue callbacks for handler2\nllm = OpenAI(temperature=0, streaming=True, callbacks=[handler2])\ntools = load_tools(["llm-math"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n)\n\n# Callbacks for handler1 will be issued by every object involved in the \n# Agent execution (llm, llmchain, tool, agent executor)\nagent.run("What is 2 raised to the 0.235 power?", callbacks=[handler1])\n')),(0,a.kt)(_,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    on_chain_start AgentExecutor\n    on_chain_start LLMChain\n    on_llm_start OpenAI\n    on_llm_start (I'm the second handler!!) OpenAI\n    on_new_token  I\n    on_new_token  need\n    on_new_token  to\n    on_new_token  use\n    on_new_token  a\n    on_new_token  calculator\n    on_new_token  to\n    on_new_token  solve\n    on_new_token  this\n    on_new_token .\n    on_new_token \n    Action\n    on_new_token :\n    on_new_token  Calculator\n    on_new_token \n    Action\n    on_new_token  Input\n    on_new_token :\n    on_new_token  2\n    on_new_token ^\n    on_new_token 0\n    on_new_token .\n    on_new_token 235\n    on_new_token \n    on_agent_action AgentAction(tool='Calculator', tool_input='2^0.235', log=' I need to use a calculator to solve this.\\nAction: Calculator\\nAction Input: 2^0.235')\n    on_tool_start Calculator\n    on_chain_start LLMMathChain\n    on_chain_start LLMChain\n    on_llm_start OpenAI\n    on_llm_start (I'm the second handler!!) OpenAI\n    on_new_token \n    on_new_token ```text\n    on_new_token \n    \n    on_new_token 2\n    on_new_token **\n    on_new_token 0\n    on_new_token .\n    on_new_token 235\n    on_new_token \n    \n    on_new_token ```\n    \n    on_new_token ...\n    on_new_token num\n    on_new_token expr\n    on_new_token .\n    on_new_token evaluate\n    on_new_token (\"\n    on_new_token 2\n    on_new_token **\n    on_new_token 0\n    on_new_token .\n    on_new_token 235\n    on_new_token \")\n    on_new_token ...\n    on_new_token \n    \n    on_new_token \n    on_chain_start LLMChain\n    on_llm_start OpenAI\n    on_llm_start (I'm the second handler!!) OpenAI\n    on_new_token  I\n    on_new_token  now\n    on_new_token  know\n    on_new_token  the\n    on_new_token  final\n    on_new_token  answer\n    on_new_token .\n    on_new_token \n    Final\n    on_new_token  Answer\n    on_new_token :\n    on_new_token  1\n    on_new_token .\n    on_new_token 17\n    on_new_token 690\n    on_new_token 67\n    on_new_token 372\n    on_new_token 187\n    on_new_token 674\n    on_new_token \n\n\n\n\n\n    '1.1769067372187674'\n"))))}u.isMDXComponent=!0}}]);